{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef6e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v_/kgpg3j1d2b725vwkr1pkjxgw0000gn/T/ipykernel_63370/3055108504.py:76: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL], utc=False, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnung: fehlen im Datensatz und werden ignoriert: ['Aussentemp_degCt', 'Globale_Strahlung_Wm2', 'Windgeschw_mps']\n",
      "Datengrößen – Gesamt: 64356, Train: 51484, Test (Holdout): 12872\n",
      "\n",
      "Baseline (Naive Persistence) – Scores:\n",
      "BL  (Train) MAE:  3.8910\n",
      "BL  (Train) RMSE: 7.5801\n",
      "BL  (Train) MAPE: 1,108.45%\n",
      "BL  (Train) R²:   0.9322\n",
      "BL  (Test)  MAE:  6.9222\n",
      "BL  (Test)  RMSE: 8.8247\n",
      "BL  (Test)  MAPE: 90.18%\n",
      "BL  (Test)  R²:   0.9102\n",
      "\n",
      "[ARIMA] Suche nach gutem (p,d,q)...\n",
      "  Order=(1, 0, 1), MAE=36.9644, Zeit=1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinkrawtzow/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Order=(2, 0, 2), MAE=33.3072, Zeit=11.0s\n",
      "  Order=(1, 1, 1), MAE=24.0711, Zeit=1.0s\n",
      "  Order=(2, 1, 1), MAE=24.0716, Zeit=1.9s\n",
      "[ARIMA] Beste Ordnung laut Val-MAE: (1, 1, 1) (MAE=24.0711)\n",
      "\n",
      "[ARIMA] Finale Anpassung auf gesamtem Trainingszeitraum...\n",
      "[ARIMA] Trainings+Forecast-Dauer (Train->Test): 2.2s\n",
      "ARIMA TST MAE:  27.6836\n",
      "ARIMA TST RMSE: 36.8271\n",
      "ARIMA TST MAPE: 254.72%\n",
      "ARIMA TST R²:   -0.5642\n",
      "\n",
      "[RF] Hyperparameter-Suche (kleines Grid, erste Optimierung)...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinkrawtzow/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 522\u001b[39m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLSTM (best_cfg=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlstm_best_cfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m, lstm_scores)\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 484\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    475\u001b[39m rf_base = RandomForestRegressor(\n\u001b[32m    476\u001b[39m     random_state=RANDOM_STATE,\n\u001b[32m    477\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    479\u001b[39m rf_param_grid = {\n\u001b[32m    480\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m200\u001b[39m, \u001b[32m400\u001b[39m],\n\u001b[32m    481\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m12\u001b[39m],\n\u001b[32m    482\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmin_samples_leaf\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m],\n\u001b[32m    483\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m rf_scores, rf_best_params, rf_pred = \u001b[43mrun_tree_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf_param_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# ===== XGBoost =====\u001b[39;00m\n\u001b[32m    489\u001b[39m xgb_base = XGBRegressor(\n\u001b[32m    490\u001b[39m     objective=\u001b[33m\"\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    491\u001b[39m     random_state=RANDOM_STATE,\n\u001b[32m   (...)\u001b[39m\u001b[32m    494\u001b[39m     eval_metric=\u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    495\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 230\u001b[39m, in \u001b[36mrun_tree_model\u001b[39m\u001b[34m(model_name, base_model, param_grid, X_train, y_train, X_test, y_test)\u001b[39m\n\u001b[32m    221\u001b[39m grid = GridSearchCV(\n\u001b[32m    222\u001b[39m     estimator=base_model,\n\u001b[32m    223\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     verbose=\u001b[32m1\u001b[39m,\n\u001b[32m    228\u001b[39m )\n\u001b[32m    229\u001b[39m start_t = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m dur = time.time() - start_t\n\u001b[32m    232\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Beste Parameter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1053\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1047\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1048\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1049\u001b[39m     )\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1057\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1612\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1610\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1611\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:999\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    992\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    995\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    996\u001b[39m         )\n\u001b[32m    997\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1022\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/sklearn/utils/parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Minimal-Setup:\n",
    "    pip install pandas numpy scikit-learn matplotlib statsmodels xgboost torch\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ======= USER-INPUT =======\n",
    "CSV_PATH = r\"/Users/martinkrawtzow/Documents/Masterthesis/thesis/Code/Github/masterthesis/src/data/raw/timeseries/SHUW_E24_hist.csv\"\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "CANDIDATE_TARGETS = [\"P_MW\"]\n",
    "RAW_FEATURES = [\"Windgeschw_mps\", \"Globale_Strahlung_Wm2\", \"Aussentemp_degCt\"]\n",
    "\n",
    "# Autoregressive Lags (Vielfache von 15 Minuten)\n",
    "TARGET_LAGS = [1, 4, 8, 12, 24, 96]  # 15min, 1h, 2h, 3h, 6h, 24h\n",
    "\n",
    "# Rolling-Window-Längen (in 15-min-Schritten)\n",
    "ROLL_WINDOWS = {\n",
    "    \"Windgeschw_mps\": [4, 12, 24],\n",
    "    \"Globale_Strahlung_Wm2\": [4, 12, 24],\n",
    "}\n",
    "\n",
    "# Fourier-Saisonalität (Tagesperiodik = 24h = 96 Schritte à 15min)\n",
    "FOURIER_K = 3\n",
    "PERIOD_STEPS = 96  # 24h\n",
    "\n",
    "HOLDOUT_FRAC = 0.2\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# LSTM-spezifisch (erstes, bewusst einfaches Setup)\n",
    "LSTM_LOOKBACK = 24   # 24 Schritte (= 6h bei 15min)\n",
    "LSTM_EPOCHS = 15\n",
    "LSTM_BATCH_SIZE = 256\n",
    "LSTM_PATIENCE = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "# ==========================\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV nicht gefunden: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    target_col = None\n",
    "    for c in CANDIDATE_TARGETS:\n",
    "        if c in df.columns:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        raise KeyError(f\"Zielspalte nicht gefunden. Erwartet eine von {CANDIDATE_TARGETS}.\")\n",
    "\n",
    "    if TIMESTAMP_COL not in df.columns:\n",
    "        raise KeyError(f\"Spalte '{TIMESTAMP_COL}' fehlt in CSV.\")\n",
    "\n",
    "    df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL], utc=False, infer_datetime_format=True)\n",
    "    df = df.sort_values(TIMESTAMP_COL).reset_index(drop=True)\n",
    "    return df, target_col\n",
    "\n",
    "\n",
    "def add_basic_time_features(df, ts_col):\n",
    "    ts = df[ts_col]\n",
    "    df[\"hour\"] = ts.dt.hour\n",
    "    df[\"dayofweek\"] = ts.dt.dayofweek\n",
    "    df[\"month\"] = ts.dt.month\n",
    "    # Zirkuläre Kodierung\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"dayofweek\"] / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"dayofweek\"] / 7)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_fourier_features(df, period_steps, K, prefix=\"day\"):\n",
    "    step_idx = np.arange(len(df), dtype=float) % period_steps\n",
    "    for k in range(1, K + 1):\n",
    "        df[f\"{prefix}_sin{k}\"] = np.sin(2 * np.pi * k * step_idx / period_steps)\n",
    "        df[f\"{prefix}_cos{k}\"] = np.cos(2 * np.pi * k * step_idx / period_steps)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_target_lags(df, target_col, lags):\n",
    "    for l in lags:\n",
    "        df[f\"{target_col}_lag{l}\"] = df[target_col].shift(l)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_features(df, windows_dict):\n",
    "    for col, wins in windows_dict.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        for w in wins:\n",
    "            df[f\"{col}_roll{w}_mean\"] = df[col].rolling(window=w, min_periods=w).mean()\n",
    "            df[f\"{col}_roll{w}_std\"] = df[col].rolling(window=w, min_periods=w).std()\n",
    "            df[f\"{col}_roll{w}_min\"] = df[col].rolling(window=w, min_periods=w).min()\n",
    "            df[f\"{col}_roll{w}_max\"] = df[col].rolling(window=w, min_periods=w).max()\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_mape(y_true, y_pred, eps=1e-6):\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred, prefix=\"\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = safe_mape(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{prefix}MAE:  {mae:,.4f}\")\n",
    "    print(f\"{prefix}RMSE: {rmse:,.4f}\")\n",
    "    print(f\"{prefix}MAPE: {mape:,.2f}%\")\n",
    "    print(f\"{prefix}R²:   {r2:,.4f}\")\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape, \"R2\": r2}\n",
    "\n",
    "\n",
    "def plot_holdout(ts, y_true, y_pred, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(ts, y_true, label=\"Ist\", linewidth=1.5)\n",
    "    plt.plot(ts, y_pred, label=\"Prognose\", linewidth=1.5, alpha=0.9)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Zeit\")\n",
    "    plt.ylabel(\"Ziel\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ===================== ARIMA =====================\n",
    "\n",
    "def select_arima_order(y_train, candidate_orders):\n",
    "    n = len(y_train)\n",
    "    split_idx = int(0.8 * n)\n",
    "    y_tr = y_train[:split_idx]\n",
    "    y_val = y_train[split_idx:]\n",
    "\n",
    "    best_order = None\n",
    "    best_mae = np.inf\n",
    "\n",
    "    print(\"\\n[ARIMA] Suche nach gutem (p,d,q)...\")\n",
    "    for order in candidate_orders:\n",
    "        try:\n",
    "            start_t = time.time()\n",
    "            model = sm.tsa.SARIMAX(\n",
    "                y_tr,\n",
    "                order=order,\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False,\n",
    "            )\n",
    "            res = model.fit(disp=False)\n",
    "            val_forecast = res.forecast(steps=len(y_val))\n",
    "            mae = mean_absolute_error(y_val, val_forecast)\n",
    "            dur = time.time() - start_t\n",
    "            print(f\"  Order={order}, MAE={mae:.4f}, Zeit={dur:.1f}s\")\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_order = order\n",
    "        except Exception as e:\n",
    "            print(f\"  Order={order} übersprungen (Fehler: {e})\")\n",
    "            continue\n",
    "\n",
    "    print(f\"[ARIMA] Beste Ordnung laut Val-MAE: {best_order} (MAE={best_mae:.4f})\")\n",
    "    return best_order\n",
    "\n",
    "\n",
    "def run_arima(y_train, y_test):\n",
    "    candidate_orders = [\n",
    "        (1, 0, 1),\n",
    "        (2, 0, 2),\n",
    "        (1, 1, 1),\n",
    "        (2, 1, 1),\n",
    "    ]\n",
    "\n",
    "    best_order = select_arima_order(y_train, candidate_orders)\n",
    "\n",
    "    print(\"\\n[ARIMA] Finale Anpassung auf gesamtem Trainingszeitraum...\")\n",
    "    start_t = time.time()\n",
    "    model = sm.tsa.SARIMAX(\n",
    "        y_train,\n",
    "        order=best_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "    )\n",
    "    res = model.fit(disp=False)\n",
    "    y_pred_test = res.forecast(steps=len(y_test))\n",
    "    dur = time.time() - start_t\n",
    "    print(f\"[ARIMA] Trainings+Forecast-Dauer (Train->Test): {dur:.1f}s\")\n",
    "\n",
    "    scores = evaluate(y_test, y_pred_test, prefix=\"ARIMA TST \")\n",
    "    return scores, best_order, y_pred_test\n",
    "\n",
    "\n",
    "# ===================== Random Forest & XGBoost =====================\n",
    "\n",
    "def run_tree_model(model_name, base_model, param_grid, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n[{model_name}] Hyperparameter-Suche (kleines Grid, erste Optimierung)...\")\n",
    "    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    start_t = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    dur = time.time() - start_t\n",
    "    print(f\"[{model_name}] Beste Parameter: {grid.best_params_}\")\n",
    "    print(f\"[{model_name}] Beste CV-MAE: {-grid.best_score_:.4f}\")\n",
    "    print(f\"[{model_name}] GridSearch-Dauer: {dur:.1f}s\")\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    scores = evaluate(y_test, y_pred_test, prefix=f\"{model_name} TST \")\n",
    "    return scores, grid.best_params_, y_pred_test\n",
    "\n",
    "\n",
    "# ===================== LSTM (PyTorch) =====================\n",
    "\n",
    "def create_lstm_dataset(series, lookback):\n",
    "    \"\"\"\n",
    "    Univariate LSTM:\n",
    "    X_t = [y_{t-lookback}, ..., y_{t-1}], y_t = Ziel.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(series)):\n",
    "        X.append(series[i - lookback:i])\n",
    "        y.append(series[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # Form: (samples, timesteps, features=1)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # letztes Zeitfenster\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_lstm_model(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    hidden_size,\n",
    "    dropout,\n",
    "    epochs=LSTM_EPOCHS,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "):\n",
    "    X_tr_t = torch.tensor(X_tr, dtype=torch.float32).to(DEVICE)\n",
    "    y_tr_t = torch.tensor(y_tr, dtype=torch.float32).unsqueeze(-1).to(DEVICE)\n",
    "    X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "    y_val_t = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1).to(DEVICE)\n",
    "\n",
    "    train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "    val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LSTMRegressor(input_size=1, hidden_size=hidden_size, dropout=dropout).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= LSTM_PATIENCE:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_val_loss\n",
    "\n",
    "\n",
    "def run_lstm(y_train, y_test, lookback=LSTM_LOOKBACK):\n",
    "    print(\"\\n[LSTM] Erzeuge Sequenzdaten...\")\n",
    "    y_full = np.concatenate([y_train, y_test])\n",
    "    X_seq, y_seq = create_lstm_dataset(y_full, lookback)\n",
    "\n",
    "    n_train = len(y_train)\n",
    "    last_train_idx = n_train - 1\n",
    "    train_seq_end = last_train_idx - lookback\n",
    "\n",
    "    X_train_seq = X_seq[: train_seq_end + 1]\n",
    "    y_train_seq = y_seq[: train_seq_end + 1]\n",
    "    X_test_seq = X_seq[train_seq_end + 1:]\n",
    "    y_test_seq = y_seq[train_seq_end + 1:]\n",
    "\n",
    "    print(f\"[LSTM] Seq-Train: {X_train_seq.shape}, Seq-Test: {X_test_seq.shape}\")\n",
    "\n",
    "    n_seq_train = len(X_train_seq)\n",
    "    val_split_idx = int(0.8 * n_seq_train)\n",
    "    X_tr, X_val = X_train_seq[:val_split_idx], X_train_seq[val_split_idx:]\n",
    "    y_tr, y_val = y_train_seq[:val_split_idx], y_train_seq[val_split_idx:]\n",
    "\n",
    "    candidate_params = [\n",
    "        {\"hidden_size\": 32, \"dropout\": 0.0},\n",
    "        {\"hidden_size\": 64, \"dropout\": 0.0},\n",
    "        {\"hidden_size\": 64, \"dropout\": 0.2},\n",
    "    ]\n",
    "\n",
    "    best_cfg = None\n",
    "    best_mae = np.inf\n",
    "\n",
    "    print(\"[LSTM] Hyperparameter-Suche (hidden_size, dropout)...\")\n",
    "    for cfg in candidate_params:\n",
    "        print(f\"  Teste cfg={cfg} ...\")\n",
    "        start_t = time.time()\n",
    "        model, best_val_loss = train_lstm_model(\n",
    "            X_tr,\n",
    "            y_tr,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            hidden_size=cfg[\"hidden_size\"],\n",
    "            dropout=cfg[\"dropout\"],\n",
    "        )\n",
    "        dur = time.time() - start_t\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "            val_pred = model(X_val_t).cpu().numpy().flatten()\n",
    "        mae = mean_absolute_error(y_val, val_pred)\n",
    "        print(f\"    -> Val-MAE={mae:.4f}, Zeit={dur:.1f}s, best_val_loss={best_val_loss:.4f}\")\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_cfg = cfg\n",
    "\n",
    "    print(f\"[LSTM] Beste Konfiguration: {best_cfg}, Val-MAE={best_mae:.4f}\")\n",
    "\n",
    "    # Finale Anpassung auf allen Seq-Train-Daten\n",
    "    print(\"[LSTM] Finale Anpassung mit bester Konfiguration...\")\n",
    "    start_t = time.time()\n",
    "    model, _ = train_lstm_model(\n",
    "        X_train_seq,\n",
    "        y_train_seq,\n",
    "        X_val=X_train_seq[int(0.9 * len(X_train_seq)):],\n",
    "        y_val=y_train_seq[int(0.9 * len(X_train_seq)):],\n",
    "        hidden_size=best_cfg[\"hidden_size\"],\n",
    "        dropout=best_cfg[\"dropout\"],\n",
    "    )\n",
    "    dur = time.time() - start_t\n",
    "    print(f\"[LSTM] Trainingsdauer gesamt: {dur:.1f}s\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_t = torch.tensor(X_test_seq, dtype=torch.float32).to(DEVICE)\n",
    "        y_pred_test_seq = model(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "    scores = evaluate(y_test_seq, y_pred_test_seq, prefix=\"LSTM TST \")\n",
    "    return scores, best_cfg, y_pred_test_seq\n",
    "\n",
    "\n",
    "# ===================== MAIN =====================\n",
    "\n",
    "def main():\n",
    "    df, target_col = load_data(CSV_PATH)\n",
    "\n",
    "    # Basis-Zeitfeatures + Fourier\n",
    "    df = add_basic_time_features(df, TIMESTAMP_COL)\n",
    "    df = add_fourier_features(df, PERIOD_STEPS, FOURIER_K, prefix=\"day\")\n",
    "\n",
    "    present_feats = [c for c in RAW_FEATURES if c in df.columns]\n",
    "    missing = sorted(set(RAW_FEATURES) - set(present_feats))\n",
    "    if missing:\n",
    "        print(f\"Warnung: fehlen im Datensatz und werden ignoriert: {missing}\")\n",
    "\n",
    "    df = add_rolling_features(df, ROLL_WINDOWS)\n",
    "    df = add_target_lags(df, target_col, TARGET_LAGS)\n",
    "\n",
    "    base_time_feats = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"month_sin\", \"month_cos\"]\n",
    "    fourier_feats = [f\"day_sin{k}\" for k in range(1, FOURIER_K + 1)] + \\\n",
    "                    [f\"day_cos{k}\" for k in range(1, FOURIER_K + 1)]\n",
    "    lag_feats = [f\"{target_col}_lag{l}\" for l in TARGET_LAGS]\n",
    "    roll_feats = [c for c in df.columns\n",
    "                  if any(c.startswith(f\"{f}_roll\") for f in ROLL_WINDOWS.keys())]\n",
    "\n",
    "    X_cols = present_feats + base_time_feats + fourier_feats + lag_feats + roll_feats\n",
    "\n",
    "    df_model = df.dropna(subset=X_cols + [target_col]).copy()\n",
    "\n",
    "    n = len(df_model)\n",
    "    split_idx = int((1.0 - HOLDOUT_FRAC) * n)\n",
    "    X = df_model[X_cols]\n",
    "    y = df_model[target_col]\n",
    "\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    print(f\"Datengrößen – Gesamt: {n}, Train: {len(X_train)}, Test (Holdout): {len(X_test)}\")\n",
    "\n",
    "    # ===== Baseline (Naive: y_hat_t = y_{t-1}) =====\n",
    "    y_train_baseline = df_model[f\"{target_col}_lag1\"].iloc[:split_idx].values\n",
    "    y_test_baseline = df_model[f\"{target_col}_lag1\"].iloc[split_idx:].values\n",
    "\n",
    "    print(\"\\nBaseline (Naive Persistence) – Scores:\")\n",
    "    _ = evaluate(y_train.values, y_train_baseline, prefix=\"BL  (Train) \")\n",
    "    bl_scores = evaluate(y_test.values, y_test_baseline, prefix=\"BL  (Test)  \")\n",
    "\n",
    "    # ===== ARIMA =====\n",
    "    arima_scores, arima_order, arima_pred = run_arima(y_train.values, y_test.values)\n",
    "\n",
    "    # ===== Random Forest =====\n",
    "    rf_base = RandomForestRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    rf_param_grid = {\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [None, 12],\n",
    "        \"min_samples_leaf\": [1, 10],\n",
    "    }\n",
    "    rf_scores, rf_best_params, rf_pred = run_tree_model(\n",
    "        \"RF\", rf_base, rf_param_grid, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # ===== XGBoost =====\n",
    "    xgb_base = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=400,\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "    xgb_param_grid = {\n",
    "        \"max_depth\": [4, 8],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "    }\n",
    "    xgb_scores, xgb_best_params, xgb_pred = run_tree_model(\n",
    "        \"XGB\", xgb_base, xgb_param_grid, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # ===== LSTM (PyTorch) =====\n",
    "    lstm_scores, lstm_best_cfg, lstm_pred = run_lstm(y_train.values, y_test.values)\n",
    "\n",
    "    # ===== Plot beispielhaft (Holdout: XGB vs Ist) =====\n",
    "    ts_holdout = df_model[TIMESTAMP_COL].iloc[split_idx:]\n",
    "    plot_holdout(ts_holdout, y_test.values, xgb_pred, title=f\"Vorhersage {target_col} – Holdout (XGB)\")\n",
    "\n",
    "    print(\"\\n=== Zusammenfassung (Test/Holdout) ===\")\n",
    "    print(\"Baseline:\", bl_scores)\n",
    "    print(f\"ARIMA (order={arima_order}):\", arima_scores)\n",
    "    print(f\"RF (best_params={rf_best_params}):\", rf_scores)\n",
    "    print(f\"XGB (best_params={xgb_best_params}):\", xgb_scores)\n",
    "    print(f\"LSTM (best_cfg={lstm_best_cfg}):\", lstm_scores)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da60310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
