{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef6e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datengrößen – Gesamt: 64356, Train: 51484, Test (Holdout): 12872\n",
      "\n",
      "Baseline (Naive Persistence) – Scores:\n",
      "BL  (Train) MAE:  3.8910\n",
      "BL  (Train) RMSE: 7.5801\n",
      "BL  (Train) MAPE: 1,108.45%\n",
      "BL  (Train) R²:   0.9322\n",
      "BL  (Test)  MAE:  6.9222\n",
      "BL  (Test)  RMSE: 8.8247\n",
      "BL  (Test)  MAPE: 90.18%\n",
      "BL  (Test)  R²:   0.9102\n",
      "\n",
      "[ARIMA] Suche nach gutem (p,d,q)...\n",
      "  Order=(1, 0, 1), MAE=36.9644, Zeit=1.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 522\u001b[39m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLSTM (best_cfg=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlstm_best_cfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m, lstm_scores)\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 472\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    469\u001b[39m bl_scores = evaluate(y_test.values, y_test_baseline, prefix=\u001b[33m\"\u001b[39m\u001b[33mBL  (Test)  \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# ===== ARIMA =====\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m arima_scores, arima_order, arima_pred = \u001b[43mrun_arima\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# ===== Random Forest =====\u001b[39;00m\n\u001b[32m    475\u001b[39m rf_base = RandomForestRegressor(\n\u001b[32m    476\u001b[39m     random_state=RANDOM_STATE,\n\u001b[32m    477\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m    478\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mrun_arima\u001b[39m\u001b[34m(y_train, y_test)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_arima\u001b[39m(y_train, y_test):\n\u001b[32m    190\u001b[39m     candidate_orders = [\n\u001b[32m    191\u001b[39m         (\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m),\n\u001b[32m    192\u001b[39m         (\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m),\n\u001b[32m    193\u001b[39m         (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m),\n\u001b[32m    194\u001b[39m         (\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m),\n\u001b[32m    195\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     best_order = \u001b[43mselect_arima_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_orders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[ARIMA] Finale Anpassung auf gesamtem Trainingszeitraum...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    200\u001b[39m     start_t = time.time()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36mselect_arima_order\u001b[39m\u001b[34m(y_train, candidate_orders)\u001b[39m\n\u001b[32m    166\u001b[39m start_t = time.time()\n\u001b[32m    167\u001b[39m model = sm.tsa.SARIMAX(\n\u001b[32m    168\u001b[39m     y_tr,\n\u001b[32m    169\u001b[39m     order=order,\n\u001b[32m    170\u001b[39m     enforce_stationarity=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    171\u001b[39m     enforce_invertibility=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    172\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m res = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m val_forecast = res.forecast(steps=\u001b[38;5;28mlen\u001b[39m(y_val))\n\u001b[32m    175\u001b[39m mae = mean_absolute_error(y_val, val_forecast)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/tsa/statespace/mlemodel.py:705\u001b[39m, in \u001b[36mMLEModel.fit\u001b[39m\u001b[34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m         flags[\u001b[33m'\u001b[39m\u001b[33mhessian_method\u001b[39m\u001b[33m'\u001b[39m] = optim_hessian\n\u001b[32m    704\u001b[39m     fargs = (flags,)\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     mlefit = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:566\u001b[39m, in \u001b[36mLikelihoodModel.fit\u001b[39m\u001b[34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[39m\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_t\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    565\u001b[39m optimizer = Optimizer()\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m xopt, retvals, optim_settings = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[32m    576\u001b[39m optim_settings.update(kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/base/optimizer.py:245\u001b[39m, in \u001b[36mOptimizer._fit\u001b[39m\u001b[34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[39m\n\u001b[32m    242\u001b[39m     fit_funcs.update(extra_fit_funcs)\n\u001b[32m    244\u001b[39m func = fit_funcs[method]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m xopt, retvals = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m optim_settings = {\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: method, \u001b[33m'\u001b[39m\u001b[33mstart_params\u001b[39m\u001b[33m'\u001b[39m: start_params,\n\u001b[32m    251\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m'\u001b[39m: maxiter, \u001b[33m'\u001b[39m\u001b[33mfull_output\u001b[39m\u001b[33m'\u001b[39m: full_output,\n\u001b[32m    252\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m'\u001b[39m: disp, \u001b[33m'\u001b[39m\u001b[33mfargs\u001b[39m\u001b[33m'\u001b[39m: fargs, \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    253\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mretall\u001b[39m\u001b[33m'\u001b[39m: retall, \u001b[33m\"\u001b[39m\u001b[33mextra_fit_funcs\u001b[39m\u001b[33m\"\u001b[39m: extra_fit_funcs}\n\u001b[32m    254\u001b[39m optim_settings.update(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/base/optimizer.py:665\u001b[39m, in \u001b[36m_fit_lbfgs\u001b[39m\u001b[34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SP_LT_116:\n\u001b[32m    664\u001b[39m     extended_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m\"\u001b[39m]=disp\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m retvals = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextended_kwargs\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[32m    677\u001b[39m     xopt, fopt, d = retvals\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_lbfgsb_py.py:281\u001b[39m, in \u001b[36mfmin_l_bfgs_b\u001b[39m\u001b[34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[39m\n\u001b[32m    269\u001b[39m callback = _wrap_callback(callback)\n\u001b[32m    270\u001b[39m opts = {\u001b[33m'\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m'\u001b[39m: disp,\n\u001b[32m    271\u001b[39m         \u001b[33m'\u001b[39m\u001b[33miprint\u001b[39m\u001b[33m'\u001b[39m: iprint,\n\u001b[32m    272\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmaxcor\u001b[39m\u001b[33m'\u001b[39m: m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcallback\u001b[39m\u001b[33m'\u001b[39m: callback,\n\u001b[32m    279\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmaxls\u001b[39m\u001b[33m'\u001b[39m: maxls}\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m                       \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m d = {\u001b[33m'\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mjac\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    284\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    285\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mfuncalls\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mnfev\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    286\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mnit\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mnit\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    287\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mwarnflag\u001b[39m\u001b[33m'\u001b[39m: res[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m]}\n\u001b[32m    288\u001b[39m f = res[\u001b[33m'\u001b[39m\u001b[33mfun\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:404\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m    403\u001b[39m \u001b[38;5;28mself\u001b[39m._update_fun()\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:366\u001b[39m, in \u001b[36mScalarFunction._update_grad\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._orig_grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_fun()\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m \u001b[38;5;28mself\u001b[39m.g = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28mself\u001b[39m.g_updated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:41\u001b[39m, in \u001b[36m_ScalarGradWrapper.__call__\u001b[39m\u001b[34m(self, x, f0, **kwds)\u001b[39m\n\u001b[32m     39\u001b[39m     g = np.atleast_1d(\u001b[38;5;28mself\u001b[39m.grad(np.copy(x), *\u001b[38;5;28mself\u001b[39m.args))\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     g, dct = \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += dct[\u001b[33m'\u001b[39m\u001b[33mnfev\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.ngev += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:593\u001b[39m, in \u001b[36mapprox_derivative\u001b[39m\u001b[34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs, full_output, workers)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m MapWrapper(workers) \u001b[38;5;28;01mas\u001b[39;00m mf:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m         J, _nfev = \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    597\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:686\u001b[39m, in \u001b[36m_dense_difference\u001b[39m\u001b[34m(fun, x0, f0, h, use_one_sided, method, workers)\u001b[39m\n\u001b[32m    684\u001b[39m f_evals = workers(fun, x_generator2(x0, h))\n\u001b[32m    685\u001b[39m dx = [(x0[i] + h[i]) - x0[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m df = \u001b[43m[\u001b[49m\u001b[43mf_eval\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf_evals\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    687\u001b[39m df_dx = [delf / delx \u001b[38;5;28;01mfor\u001b[39;00m delf, delx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df, dx)]\n\u001b[32m    688\u001b[39m nfev += \u001b[38;5;28mlen\u001b[39m(df_dx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:879\u001b[39m, in \u001b[36m_Fun_Wrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xp.isdtype(x.dtype, \u001b[33m\"\u001b[39m\u001b[33mreal floating\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    877\u001b[39m     x = xp.astype(x, \u001b[38;5;28mself\u001b[39m.x0.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m f = np.atleast_1d(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`fun` return value has \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    882\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mmore than 1 dimension.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/scipy/_lib/_util.py:590\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:534\u001b[39m, in \u001b[36mLikelihoodModel.fit.<locals>.f\u001b[39m\u001b[34m(params, *args)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(params, *args):\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m / nobs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/tsa/statespace/mlemodel.py:940\u001b[39m, in \u001b[36mMLEModel.loglike\u001b[39m\u001b[34m(self, params, *args, **kwargs)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[32m    938\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33minversion_method\u001b[39m\u001b[33m'\u001b[39m] = INVERT_UNIVARIATE | SOLVE_LU\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m loglike = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/tsa/statespace/kalman_filter.py:1001\u001b[39m, in \u001b[36mKalmanFilter.loglike\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[32m    987\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    997\u001b[39m \u001b[33;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[32m    998\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    999\u001b[39m kwargs.setdefault(\u001b[33m'\u001b[39m\u001b[33mconserve_memory\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1000\u001b[39m                   MEMORY_CONSERVE ^ MEMORY_NO_LIKELIHOOD)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m kfilter = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m loglikelihood_burn = kwargs.get(\u001b[33m'\u001b[39m\u001b[33mloglikelihood_burn\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1003\u001b[39m                                 \u001b[38;5;28mself\u001b[39m.loglikelihood_burn)\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[33m'\u001b[39m\u001b[33mconserve_memory\u001b[39m\u001b[33m'\u001b[39m] & MEMORY_NO_LIKELIHOOD):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Masterthesis/thesis/Code/Github/masterthesis/.venv/lib/python3.13/site-packages/statsmodels/tsa/statespace/kalman_filter.py:924\u001b[39m, in \u001b[36mKalmanFilter._filter\u001b[39m\u001b[34m(self, filter_method, inversion_method, stability_method, conserve_memory, filter_timing, tolerance, loglikelihood_burn, complex_step)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_state(prefix=prefix, complex_step=complex_step)\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m \u001b[43mkfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m kfilter\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Minimal-Setup:\n",
    "    pip install pandas numpy scikit-learn matplotlib statsmodels xgboost torch\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ======= USER-INPUT =======\n",
    "CSV_PATH = r\"/Users/martinkrawtzow/Documents/Masterthesis/thesis/Code/Github/masterthesis/src/data/raw/timeseries/SHUW_E24_hist.csv\"\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "CANDIDATE_TARGETS = [\"P_MW\"]\n",
    "RAW_FEATURES = [\"wind_speed_mps\", \"solar_radiation_Wm2\", \"temperature_C\"]\n",
    "\n",
    "# Autoregressive Lags (Vielfache von 15 Minuten)\n",
    "TARGET_LAGS = [1, 4, 8, 12, 24, 96]  # 15min, 1h, 2h, 3h, 6h, 24h\n",
    "\n",
    "# Rolling-Window-Längen (in 15-min-Schritten)\n",
    "ROLL_WINDOWS = {\n",
    "    \"Windgeschw_mps\": [4, 12, 24],\n",
    "    \"Globale_Strahlung_Wm2\": [4, 12, 24],\n",
    "}\n",
    "\n",
    "# Fourier-Saisonalität (Tagesperiodik = 24h = 96 Schritte à 15min)\n",
    "FOURIER_K = 3\n",
    "PERIOD_STEPS = 96  # 24h\n",
    "\n",
    "HOLDOUT_FRAC = 0.2\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# LSTM-spezifisch (erstes, bewusst einfaches Setup)\n",
    "LSTM_LOOKBACK = 24   # 24 Schritte (= 6h bei 15min)\n",
    "LSTM_EPOCHS = 15\n",
    "LSTM_BATCH_SIZE = 256\n",
    "LSTM_PATIENCE = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "# ==========================\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV nicht gefunden: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    target_col = None\n",
    "    for c in CANDIDATE_TARGETS:\n",
    "        if c in df.columns:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        raise KeyError(f\"Zielspalte nicht gefunden. Erwartet eine von {CANDIDATE_TARGETS}.\")\n",
    "\n",
    "    if TIMESTAMP_COL not in df.columns:\n",
    "        raise KeyError(f\"Spalte '{TIMESTAMP_COL}' fehlt in CSV.\")\n",
    "\n",
    "    df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL], utc=False)\n",
    "    df = df.sort_values(TIMESTAMP_COL).reset_index(drop=True)\n",
    "    return df, target_col\n",
    "\n",
    "\n",
    "def add_basic_time_features(df, ts_col):\n",
    "    ts = df[ts_col]\n",
    "    df[\"hour\"] = ts.dt.hour\n",
    "    df[\"dayofweek\"] = ts.dt.dayofweek\n",
    "    df[\"month\"] = ts.dt.month\n",
    "    # Zirkuläre Kodierung\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"dayofweek\"] / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"dayofweek\"] / 7)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_fourier_features(df, period_steps, K, prefix=\"day\"):\n",
    "    step_idx = np.arange(len(df), dtype=float) % period_steps\n",
    "    for k in range(1, K + 1):\n",
    "        df[f\"{prefix}_sin{k}\"] = np.sin(2 * np.pi * k * step_idx / period_steps)\n",
    "        df[f\"{prefix}_cos{k}\"] = np.cos(2 * np.pi * k * step_idx / period_steps)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_target_lags(df, target_col, lags):\n",
    "    for l in lags:\n",
    "        df[f\"{target_col}_lag{l}\"] = df[target_col].shift(l)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_features(df, windows_dict):\n",
    "    for col, wins in windows_dict.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        for w in wins:\n",
    "            df[f\"{col}_roll{w}_mean\"] = df[col].rolling(window=w, min_periods=w).mean()\n",
    "            df[f\"{col}_roll{w}_std\"] = df[col].rolling(window=w, min_periods=w).std()\n",
    "            df[f\"{col}_roll{w}_min\"] = df[col].rolling(window=w, min_periods=w).min()\n",
    "            df[f\"{col}_roll{w}_max\"] = df[col].rolling(window=w, min_periods=w).max()\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_mape(y_true, y_pred, eps=1e-6):\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred, prefix=\"\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = safe_mape(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{prefix}MAE:  {mae:,.4f}\")\n",
    "    print(f\"{prefix}RMSE: {rmse:,.4f}\")\n",
    "    print(f\"{prefix}MAPE: {mape:,.2f}%\")\n",
    "    print(f\"{prefix}R²:   {r2:,.4f}\")\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape, \"R2\": r2}\n",
    "\n",
    "\n",
    "def plot_holdout(ts, y_true, y_pred, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(ts, y_true, label=\"Ist\", linewidth=1.5)\n",
    "    plt.plot(ts, y_pred, label=\"Prognose\", linewidth=1.5, alpha=0.9)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Zeit\")\n",
    "    plt.ylabel(\"Ziel\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ===================== ARIMA =====================\n",
    "\n",
    "def select_arima_order(y_train, candidate_orders):\n",
    "    n = len(y_train)\n",
    "    split_idx = int(0.8 * n)\n",
    "    y_tr = y_train[:split_idx]\n",
    "    y_val = y_train[split_idx:]\n",
    "\n",
    "    best_order = None\n",
    "    best_mae = np.inf\n",
    "\n",
    "    print(\"\\n[ARIMA] Suche nach gutem (p,d,q)...\")\n",
    "    for order in candidate_orders:\n",
    "        try:\n",
    "            start_t = time.time()\n",
    "            model = sm.tsa.SARIMAX(\n",
    "                y_tr,\n",
    "                order=order,\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False,\n",
    "            )\n",
    "            res = model.fit(disp=False)\n",
    "            val_forecast = res.forecast(steps=len(y_val))\n",
    "            mae = mean_absolute_error(y_val, val_forecast)\n",
    "            dur = time.time() - start_t\n",
    "            print(f\"  Order={order}, MAE={mae:.4f}, Zeit={dur:.1f}s\")\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_order = order\n",
    "        except Exception as e:\n",
    "            print(f\"  Order={order} übersprungen (Fehler: {e})\")\n",
    "            continue\n",
    "\n",
    "    print(f\"[ARIMA] Beste Ordnung laut Val-MAE: {best_order} (MAE={best_mae:.4f})\")\n",
    "    return best_order\n",
    "\n",
    "\n",
    "def run_arima(y_train, y_test):\n",
    "    candidate_orders = [\n",
    "        (1, 0, 1),\n",
    "        (2, 0, 2),\n",
    "        (1, 1, 1),\n",
    "        (2, 1, 1),\n",
    "    ]\n",
    "\n",
    "    best_order = select_arima_order(y_train, candidate_orders)\n",
    "\n",
    "    print(\"\\n[ARIMA] Finale Anpassung auf gesamtem Trainingszeitraum...\")\n",
    "    start_t = time.time()\n",
    "    model = sm.tsa.SARIMAX(\n",
    "        y_train,\n",
    "        order=best_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "    )\n",
    "    res = model.fit(disp=False)\n",
    "    y_pred_test = res.forecast(steps=len(y_test))\n",
    "    dur = time.time() - start_t\n",
    "    print(f\"[ARIMA] Trainings+Forecast-Dauer (Train->Test): {dur:.1f}s\")\n",
    "\n",
    "    scores = evaluate(y_test, y_pred_test, prefix=\"ARIMA TST \")\n",
    "    return scores, best_order, y_pred_test\n",
    "\n",
    "\n",
    "# ===================== Random Forest & XGBoost =====================\n",
    "\n",
    "def run_tree_model(model_name, base_model, param_grid, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n[{model_name}] Hyperparameter-Suche (kleines Grid, erste Optimierung)...\")\n",
    "    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    start_t = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    dur = time.time() - start_t\n",
    "    print(f\"[{model_name}] Beste Parameter: {grid.best_params_}\")\n",
    "    print(f\"[{model_name}] Beste CV-MAE: {-grid.best_score_:.4f}\")\n",
    "    print(f\"[{model_name}] GridSearch-Dauer: {dur:.1f}s\")\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    scores = evaluate(y_test, y_pred_test, prefix=f\"{model_name} TST \")\n",
    "    return scores, grid.best_params_, y_pred_test\n",
    "\n",
    "\n",
    "# ===================== LSTM (PyTorch) =====================\n",
    "\n",
    "def create_lstm_dataset(series, lookback):\n",
    "    \"\"\"\n",
    "    Univariate LSTM:\n",
    "    X_t = [y_{t-lookback}, ..., y_{t-1}], y_t = Ziel.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(series)):\n",
    "        X.append(series[i - lookback:i])\n",
    "        y.append(series[i])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # Form: (samples, timesteps, features=1)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # letztes Zeitfenster\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_lstm_model(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    hidden_size,\n",
    "    dropout,\n",
    "    epochs=LSTM_EPOCHS,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "):\n",
    "    X_tr_t = torch.tensor(X_tr, dtype=torch.float32).to(DEVICE)\n",
    "    y_tr_t = torch.tensor(y_tr, dtype=torch.float32).unsqueeze(-1).to(DEVICE)\n",
    "    X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "    y_val_t = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1).to(DEVICE)\n",
    "\n",
    "    train_ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "    val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LSTMRegressor(input_size=1, hidden_size=hidden_size, dropout=dropout).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= LSTM_PATIENCE:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_val_loss\n",
    "\n",
    "\n",
    "def run_lstm(y_train, y_test, lookback=LSTM_LOOKBACK):\n",
    "    print(\"\\n[LSTM] Erzeuge Sequenzdaten...\")\n",
    "    y_full = np.concatenate([y_train, y_test])\n",
    "    X_seq, y_seq = create_lstm_dataset(y_full, lookback)\n",
    "\n",
    "    n_train = len(y_train)\n",
    "    last_train_idx = n_train - 1\n",
    "    train_seq_end = last_train_idx - lookback\n",
    "\n",
    "    X_train_seq = X_seq[: train_seq_end + 1]\n",
    "    y_train_seq = y_seq[: train_seq_end + 1]\n",
    "    X_test_seq = X_seq[train_seq_end + 1:]\n",
    "    y_test_seq = y_seq[train_seq_end + 1:]\n",
    "\n",
    "    print(f\"[LSTM] Seq-Train: {X_train_seq.shape}, Seq-Test: {X_test_seq.shape}\")\n",
    "\n",
    "    n_seq_train = len(X_train_seq)\n",
    "    val_split_idx = int(0.8 * n_seq_train)\n",
    "    X_tr, X_val = X_train_seq[:val_split_idx], X_train_seq[val_split_idx:]\n",
    "    y_tr, y_val = y_train_seq[:val_split_idx], y_train_seq[val_split_idx:]\n",
    "\n",
    "    candidate_params = [\n",
    "        {\"hidden_size\": 32, \"dropout\": 0.0},\n",
    "        {\"hidden_size\": 64, \"dropout\": 0.0},\n",
    "        {\"hidden_size\": 64, \"dropout\": 0.2},\n",
    "    ]\n",
    "\n",
    "    best_cfg = None\n",
    "    best_mae = np.inf\n",
    "\n",
    "    print(\"[LSTM] Hyperparameter-Suche (hidden_size, dropout)...\")\n",
    "    for cfg in candidate_params:\n",
    "        print(f\"  Teste cfg={cfg} ...\")\n",
    "        start_t = time.time()\n",
    "        model, best_val_loss = train_lstm_model(\n",
    "            X_tr,\n",
    "            y_tr,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            hidden_size=cfg[\"hidden_size\"],\n",
    "            dropout=cfg[\"dropout\"],\n",
    "        )\n",
    "        dur = time.time() - start_t\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "            val_pred = model(X_val_t).cpu().numpy().flatten()\n",
    "        mae = mean_absolute_error(y_val, val_pred)\n",
    "        print(f\"    -> Val-MAE={mae:.4f}, Zeit={dur:.1f}s, best_val_loss={best_val_loss:.4f}\")\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_cfg = cfg\n",
    "\n",
    "    print(f\"[LSTM] Beste Konfiguration: {best_cfg}, Val-MAE={best_mae:.4f}\")\n",
    "\n",
    "    # Finale Anpassung auf allen Seq-Train-Daten\n",
    "    print(\"[LSTM] Finale Anpassung mit bester Konfiguration...\")\n",
    "    start_t = time.time()\n",
    "    model, _ = train_lstm_model(\n",
    "        X_train_seq,\n",
    "        y_train_seq,\n",
    "        X_val=X_train_seq[int(0.9 * len(X_train_seq)):],\n",
    "        y_val=y_train_seq[int(0.9 * len(X_train_seq)):],\n",
    "        hidden_size=best_cfg[\"hidden_size\"],\n",
    "        dropout=best_cfg[\"dropout\"],\n",
    "    )\n",
    "    dur = time.time() - start_t\n",
    "    print(f\"[LSTM] Trainingsdauer gesamt: {dur:.1f}s\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_t = torch.tensor(X_test_seq, dtype=torch.float32).to(DEVICE)\n",
    "        y_pred_test_seq = model(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "    scores = evaluate(y_test_seq, y_pred_test_seq, prefix=\"LSTM TST \")\n",
    "    return scores, best_cfg, y_pred_test_seq\n",
    "\n",
    "\n",
    "# ===================== MAIN =====================\n",
    "\n",
    "def main():\n",
    "    df, target_col = load_data(CSV_PATH)\n",
    "\n",
    "    # Basis-Zeitfeatures + Fourier\n",
    "    df = add_basic_time_features(df, TIMESTAMP_COL)\n",
    "    df = add_fourier_features(df, PERIOD_STEPS, FOURIER_K, prefix=\"day\")\n",
    "\n",
    "    present_feats = [c for c in RAW_FEATURES if c in df.columns]\n",
    "    missing = sorted(set(RAW_FEATURES) - set(present_feats))\n",
    "    if missing:\n",
    "        print(f\"Warnung: fehlen im Datensatz und werden ignoriert: {missing}\")\n",
    "\n",
    "    df = add_rolling_features(df, ROLL_WINDOWS)\n",
    "    df = add_target_lags(df, target_col, TARGET_LAGS)\n",
    "\n",
    "    base_time_feats = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"month_sin\", \"month_cos\"]\n",
    "    fourier_feats = [f\"day_sin{k}\" for k in range(1, FOURIER_K + 1)] + \\\n",
    "                    [f\"day_cos{k}\" for k in range(1, FOURIER_K + 1)]\n",
    "    lag_feats = [f\"{target_col}_lag{l}\" for l in TARGET_LAGS]\n",
    "    roll_feats = [c for c in df.columns\n",
    "                  if any(c.startswith(f\"{f}_roll\") for f in ROLL_WINDOWS.keys())]\n",
    "\n",
    "    X_cols = present_feats + base_time_feats + fourier_feats + lag_feats + roll_feats\n",
    "\n",
    "    df_model = df.dropna(subset=X_cols + [target_col]).copy()\n",
    "\n",
    "    n = len(df_model)\n",
    "    split_idx = int((1.0 - HOLDOUT_FRAC) * n)\n",
    "    X = df_model[X_cols]\n",
    "    y = df_model[target_col]\n",
    "\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    print(f\"Datengrößen – Gesamt: {n}, Train: {len(X_train)}, Test (Holdout): {len(X_test)}\")\n",
    "\n",
    "    # ===== Baseline (Naive: y_hat_t = y_{t-1}) =====\n",
    "    y_train_baseline = df_model[f\"{target_col}_lag1\"].iloc[:split_idx].values\n",
    "    y_test_baseline = df_model[f\"{target_col}_lag1\"].iloc[split_idx:].values\n",
    "\n",
    "    print(\"\\nBaseline (Naive Persistence) – Scores:\")\n",
    "    _ = evaluate(y_train.values, y_train_baseline, prefix=\"BL  (Train) \")\n",
    "    bl_scores = evaluate(y_test.values, y_test_baseline, prefix=\"BL  (Test)  \")\n",
    "\n",
    "    # ===== ARIMA =====\n",
    "    arima_scores, arima_order, arima_pred = run_arima(y_train.values, y_test.values)\n",
    "\n",
    "    # ===== Random Forest =====\n",
    "    rf_base = RandomForestRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    rf_param_grid = {\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [None, 12],\n",
    "        \"min_samples_leaf\": [1, 10],\n",
    "    }\n",
    "    rf_scores, rf_best_params, rf_pred = run_tree_model(\n",
    "        \"RF\", rf_base, rf_param_grid, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # ===== XGBoost =====\n",
    "    xgb_base = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=400,\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "    xgb_param_grid = {\n",
    "        \"max_depth\": [4, 8],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "    }\n",
    "    xgb_scores, xgb_best_params, xgb_pred = run_tree_model(\n",
    "        \"XGB\", xgb_base, xgb_param_grid, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # ===== LSTM (PyTorch) =====\n",
    "    lstm_scores, lstm_best_cfg, lstm_pred = run_lstm(y_train.values, y_test.values)\n",
    "\n",
    "    # ===== Plot beispielhaft (Holdout: XGB vs Ist) =====\n",
    "    ts_holdout = df_model[TIMESTAMP_COL].iloc[split_idx:]\n",
    "    plot_holdout(ts_holdout, y_test.values, xgb_pred, title=f\"Vorhersage {target_col} – Holdout (XGB)\")\n",
    "\n",
    "    print(\"\\n=== Zusammenfassung (Test/Holdout) ===\")\n",
    "    print(\"Baseline:\", bl_scores)\n",
    "    print(f\"ARIMA (order={arima_order}):\", arima_scores)\n",
    "    print(f\"RF (best_params={rf_best_params}):\", rf_scores)\n",
    "    print(f\"XGB (best_params={xgb_best_params}):\", xgb_scores)\n",
    "    print(f\"LSTM (best_cfg={lstm_best_cfg}):\", lstm_scores)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da60310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
