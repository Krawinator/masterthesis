{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee376f6e",
   "metadata": {},
   "source": [
    "# Maybe final form for deriving uw fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dd7086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JUBO_E01: neu erstellt → 1344 Zeilen.\n",
      "Fertig.\n"
     ]
    }
   ],
   "source": [
    "# ===== Konfiguration ===========================================================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Pfade anpassen:\n",
    "GRAPH_PATH = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\graph\\graph_with_jubo_e01.json\")\n",
    "CSV_DIR    = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\handling_graph\\out\\nodes\")\n",
    "\n",
    "# Spaltennamen / Physik\n",
    "FEATURE_P     = \"P_Datapoint_ID\"\n",
    "FEATURE_Q     = \"Q_Datapoint_ID\"\n",
    "WEATHER_COLS  = [\"Windgeschw_Datapoint\", \"Globale_Strahlung_Datapoint\", \"Aussentemp_Datapoint\"]\n",
    "COS_PHI       = 0.95     # fixer Leistungsfaktor\n",
    "GRID_FREQ     = \"15min\"  # fixes Raster\n",
    "TIMEZONE      = None     # z.B. \"Europe/Berlin\", ansonsten None (tz-naiv)\n",
    "FORCE_REBUILD = False    # True = CSVs der derived Nodes komplett neu aufbauen\n",
    "VERBOSE       = True\n",
    "\n",
    "def log(msg: str):\n",
    "    if VERBOSE:\n",
    "        print(msg)\n",
    "\n",
    "# ===== Helpers: Zeitindex normalisieren & Duplikate zusammenfassen ============\n",
    "def _standardize_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Index TZ-bereinigen, auf 15-min runden, Duplikate pro Timestamp konsolidieren (letzter gültiger Wert).\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    idx = pd.to_datetime(df.index)\n",
    "\n",
    "    # TZ vereinheitlichen\n",
    "    if getattr(idx, \"tz\", None) is not None:\n",
    "        if TIMEZONE:\n",
    "            idx = idx.tz_convert(TIMEZONE)\n",
    "        idx = idx.tz_localize(None)\n",
    "\n",
    "    # Auf Raster runden\n",
    "    if GRID_FREQ:\n",
    "        idx = idx.round(GRID_FREQ)\n",
    "\n",
    "    df = df.copy()\n",
    "    df.index = idx\n",
    "\n",
    "    # Duplikate je Timestamp: letzter gültiger Wert pro Spalte\n",
    "    def _combine(col):\n",
    "        col = col.dropna()\n",
    "        return col.iloc[-1] if len(col) else np.nan\n",
    "\n",
    "    df = df.groupby(df.index).agg(_combine)\n",
    "    return df.sort_index()\n",
    "\n",
    "def _dedupe_merge(old: Optional[pd.DataFrame], new: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Alte + neue Daten zusammenführen und standardisieren.\"\"\"\n",
    "    if old is None or old.empty:\n",
    "        return _standardize_index(new)\n",
    "    out = pd.concat([old, new]).sort_index()\n",
    "    return _standardize_index(out)\n",
    "\n",
    "# ===== Graph laden & derived-Spezifikationen ==================================\n",
    "def read_graph(path: Path) -> list[dict]:\n",
    "    elems = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    if not isinstance(elems, list):\n",
    "        raise ValueError(\"graph.json muss eine Liste sein.\")\n",
    "    return elems\n",
    "\n",
    "def split_nodes_edges(elements: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "    nodes = [e for e in elements if \"id\" in e.get(\"data\", {}) and \"source\" not in e.get(\"data\", {})]\n",
    "    edges = [e for e in elements if \"source\" in e.get(\"data\", {}) and \"target\" in e.get(\"data\", {})]\n",
    "    return nodes, edges\n",
    "\n",
    "elements = read_graph(GRAPH_PATH)\n",
    "nodes, edges = split_nodes_edges(elements)\n",
    "nodes_by_id: Dict[str, dict] = {n[\"data\"][\"id\"]: n for n in nodes}\n",
    "\n",
    "def ntype(nid: str) -> str:\n",
    "    return (nodes_by_id.get(nid, {}).get(\"data\", {}).get(\"type\") or \"\").strip()\n",
    "\n",
    "def feats(nid: str) -> dict:\n",
    "    return (nodes_by_id.get(nid, {}).get(\"data\", {}).get(\"features\") or {}) or {}\n",
    "\n",
    "def collect_derived_specs() -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Sucht in features.derived: method=field_sum, terms=[{\"node\": \"...\", \"coeff\": ...}, ...], bias(optional).\n",
    "    Nur für type=uw_field.\n",
    "    \"\"\"\n",
    "    specs = {}\n",
    "    for nid, node in nodes_by_id.items():\n",
    "        if ntype(nid) != \"uw_field\":\n",
    "            continue\n",
    "        d = feats(nid).get(\"derived\")\n",
    "        if not d or d.get(\"method\") != \"field_sum\":\n",
    "            continue\n",
    "        terms = []\n",
    "        for t in (d.get(\"terms\") or []):\n",
    "            if isinstance(t, dict) and \"node\" in t:\n",
    "                terms.append((t[\"node\"], float(t.get(\"coeff\", 1.0))))\n",
    "            elif isinstance(t, str):\n",
    "                terms.append((t, 1.0))\n",
    "        if not terms:\n",
    "            continue\n",
    "        specs[nid] = {\"terms\": terms, \"bias\": float(d.get(\"bias\", 0.0) or 0.0)}\n",
    "    return specs\n",
    "\n",
    "derived_specs = collect_derived_specs()\n",
    "if not derived_specs:\n",
    "    log(\"Keine derived UW-Felder gefunden (features.derived).\")\n",
    "\n",
    "# ===== CSV I/O pro Node ========================================================\n",
    "_csv_cache: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "def load_node_csv(node_id: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"CSV laden, Index standardisieren (15min, Dedupe). Erwartet Spalte 'timestamp'.\"\"\"\n",
    "    if node_id in _csv_cache:\n",
    "        return _csv_cache[node_id]\n",
    "    path = CSV_DIR / f\"{node_id}.csv\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(path)\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        cand = next((c for c in df.columns if \"time\" in c.lower() or \"ts\" in c.lower()), None)\n",
    "        if cand is None:\n",
    "            raise ValueError(f\"{path} hat keine 'timestamp'-Spalte.\")\n",
    "        df = df.rename(columns={cand: \"timestamp\"})\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.set_index(\"timestamp\").sort_index()\n",
    "    df = _standardize_index(df)\n",
    "    _csv_cache[node_id] = df\n",
    "    return df\n",
    "\n",
    "def save_node_csv(node_id: str, df: pd.DataFrame):\n",
    "    path = CSV_DIR / f\"{node_id}.csv\"\n",
    "    df.to_csv(path, index_label=\"timestamp\")\n",
    "\n",
    "# ===== Wetterquelle bestimmen ==================================================\n",
    "def pick_weather_source(target_id: str) -> Optional[str]:\n",
    "    spec = derived_specs[target_id]\n",
    "    for src_id, _ in spec[\"terms\"]:\n",
    "        df = load_node_csv(src_id)\n",
    "        if df is not None and all(col in df.columns for col in WEATHER_COLS):\n",
    "            return src_id\n",
    "    return spec[\"terms\"][0][0]  # Fallback\n",
    "\n",
    "# ===== Kern: CSV für derived UW-Feld erzeugen/aktualisieren ====================\n",
    "def update_or_build_derived_csv(target_id: str, force_rebuild: bool = False):\n",
    "    \"\"\"\n",
    "    Erstellt/aktualisiert die CSV-Datei für ein derived UW-Feld.\n",
    "      - Annahme: alle CSVs sind exakt auf 15-min gerastert.\n",
    "      - P = gewichtete Summe der Quellen + Bias\n",
    "      - Q = P * tan(arccos(cosφ))\n",
    "      - Wetter = 1:1 von geeigneter Quelle\n",
    "      - Inkrementell: nur neue Timestamps anhängen, wenn force_rebuild=False\n",
    "    \"\"\"\n",
    "    spec = derived_specs[target_id]\n",
    "    terms = spec[\"terms\"]\n",
    "    bias  = float(spec.get(\"bias\", 0.0))\n",
    "\n",
    "    # Referenzquelle = erste Quelle\n",
    "    ref_id = terms[0][0]\n",
    "    ref_df = load_node_csv(ref_id)\n",
    "    if ref_df is None or ref_df.empty:\n",
    "        log(f\"⚠️ {target_id}: Referenzquelle {ref_id} nicht gefunden oder leer.\")\n",
    "        return\n",
    "\n",
    "    # Ziel-CSV (Altbestand)\n",
    "    tgt_old = load_node_csv(target_id)\n",
    "    existed = tgt_old is not None and not tgt_old.empty and not force_rebuild\n",
    "    last_ts = tgt_old.index.max() if existed else None\n",
    "\n",
    "    # Zeitindex (15-min) bestimmen\n",
    "    idx = ref_df.index\n",
    "    if existed:\n",
    "        idx = idx[idx > last_ts]\n",
    "        if idx.empty:\n",
    "            log(f\"= {target_id}: bereits aktuell ({len(tgt_old)} Zeilen).\")\n",
    "            return\n",
    "\n",
    "    # P aus Quellen bilden (gleicher Index)\n",
    "    df_P = pd.DataFrame(index=idx)\n",
    "    for src_id, coeff in terms:\n",
    "        src_df = load_node_csv(src_id)\n",
    "        if src_df is None or FEATURE_P not in src_df.columns:\n",
    "            log(f\"⚠️ {target_id}: Quelle {src_id} ohne {FEATURE_P} – setze NaN.\")\n",
    "            df_P[src_id] = np.nan\n",
    "        else:\n",
    "            # direkter Zugriff: gleiche 15-min Timestamps\n",
    "            df_P[src_id] = src_df.loc[idx, FEATURE_P].astype(float) * coeff\n",
    "\n",
    "    df_P[FEATURE_P] = df_P.sum(axis=1) + bias\n",
    "\n",
    "    # Q aus P\n",
    "    phi = np.arccos(COS_PHI)\n",
    "    tan_phi = np.tan(phi)\n",
    "    df_P[FEATURE_Q] = df_P[FEATURE_P] * tan_phi\n",
    "\n",
    "    # Wetterspalten übernehmen (1:1)\n",
    "    wsrc = pick_weather_source(target_id)\n",
    "    wdf  = load_node_csv(wsrc)\n",
    "    for col in WEATHER_COLS:\n",
    "        if wdf is not None and col in wdf.columns:\n",
    "            df_P[col] = wdf.loc[idx, col]\n",
    "        else:\n",
    "            df_P[col] = np.nan\n",
    "\n",
    "    # Statische Felder aus Features (optional mitführen)\n",
    "    f = feats(target_id)\n",
    "    if \"Strom_Limit_in_A\" in f:\n",
    "        df_P[\"Strom_Limit_in_A\"] = f[\"Strom_Limit_in_A\"]\n",
    "    if \"DAB_ID\" in f:\n",
    "        df_P[\"DAB_ID\"] = f[\"DAB_ID\"]\n",
    "\n",
    "    # Standardisieren (Dedupe/Raster – idempotent)\n",
    "    df_P = _standardize_index(df_P)\n",
    "\n",
    "    # Anhängen oder neu schreiben\n",
    "    if existed:\n",
    "        out_df = _dedupe_merge(tgt_old, df_P)\n",
    "        save_node_csv(target_id, out_df)\n",
    "        log(f\"✅ {target_id}: angehängt → +{len(df_P)} / gesamt {len(out_df)} Zeilen.\")\n",
    "    else:\n",
    "        save_node_csv(target_id, df_P)\n",
    "        log(f\"✅ {target_id}: neu erstellt → {len(df_P)} Zeilen.\")\n",
    "\n",
    "# ===== Ausführen: alle derived UW-Felder aktualisieren =========================\n",
    "if not derived_specs:\n",
    "    log(\"Nichts zu tun (keine derived-Nodes gefunden).\")\n",
    "else:\n",
    "    for target in sorted(derived_specs.keys()):\n",
    "        update_or_build_derived_csv(target, force_rebuild=FORCE_REBUILD)\n",
    "    log(\"Fertig.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
