{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58ca5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1 (MVP-Teil):\n",
    "- Graph (JSON) einlesen\n",
    "- Messwerte (CSV-Ordner; Dateien heißen wie Node-IDs) einlesen\n",
    "- Orientation: Verbraucherzähpfeilsystem (Fluss weg von der Sammelschiene = positiv)\n",
    "- Einfache Sensitivitätsanalyse: Für jeden UW-Feeder (Kante ab Sammelschiene) wird der gemessene Feederstrom I_f(t)\n",
    "  per Ridge-Regression auf die (downstream) Knotennetzleistungen P_b(t) regrediert:\n",
    "      I_f(t) ≈ α_f + Σ_b S[f,b] · P_b(t)\n",
    "  → S liefert eine (empirische) Sensitivität in A/kW (oder in den Einheiten der Eingangsdaten).\n",
    "\n",
    "Annahmen (MVP):\n",
    "- Netz ist radial oder zumindest die Downstream-Zuordnung je Feeder ist eindeutig (für Regression verwenden wir ohnehin nur die topologisch\n",
    "  zugeordneten Knoten pro Feeder). Falls Vermaschungen existieren, kann das Verfahren dennoch laufen, die Topologie-Maske dient nur als Feature-Filter.\n",
    "- CSV-Format: Spalten: timestamp, P (Leistung, Vorzeichen gemäß Verbraucherzähpfeilsystem: Einspeisung in Netz positiv)\n",
    "- Zeitachsen: Wir schneiden auf den Schnitt der Zeitstempel aller benötigten Serien.\n",
    "- Graph-JSON: {\"nodes\":[{\"id\":\"S\",\"type\":\"substation\"},...], \"edges\":[{\"id\":\"L1\",\"from\":\"S\",\"to\":\"A\"}, ...]}\n",
    "\n",
    "Ausgabe:\n",
    "- sensitivities.csv: Matrix S (Zeilen: Feeder-IDs = Edge-IDs ab Substation; Spalten: Node-IDs downstream)\n",
    "- report.txt: kurze Metriken (R² je Feeder, Anzahl verwendeter Knoten, n_samples)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "STEP 1 (iterativ & auskunftsorientiert):\n",
    "- Ziel: Alle nötigen Informationen AUSGEBEN, um eine erste Sensitivitätstabelle zu bestimmen.\n",
    "- Vorgehen:\n",
    "  1) Graph laden und Feeder + Downstream-Knoten ermitteln.\n",
    "  2) Prüfen, welche Messdateien vorhanden sind (Nodes & Feeder), welche fehlen.\n",
    "  3) Zeitachsen-Überlappung je Feeder (Schnittmenge) und Stichprobengröße ausgeben.\n",
    "  4) Falls ausreichend Daten vorhanden: einfache Ridge-Regression je Feeder durchführen und Sensitivitätstabelle schreiben.\n",
    "\n",
    "Konfiguration unten anpassen (Pfade, Spaltennamen, Schwellenwerte).\n",
    "Ausgaben im Terminal sind so strukturiert, dass du sofort siehst, wo etwas fehlt.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# ----------------------------\n",
    "# Konfiguration\n",
    "# ----------------------------\n",
    "GRAPH_JSON = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\graph\\graph_with_jubo_e01.json\")\n",
    "MEAS_DIR    = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\handling_graph\\out\\nodes\")   # CSVs, je Datei: <node_id>.csv mit Spalten [timestamp, P]\n",
    "VAL_COL = \"P_Datapoint_ID\"                     # Spaltenname in CSVs\n",
    "TS_COL      = \"timestamp\"\n",
    "ALPHA_RIDGE = 1.0                     # kleine Regularisierung für Stabilität\n",
    "ROOT_ID     = None                    # falls None: wird aus node.type==\"substation\" gelesen\n",
    "OUT_DIR     = Path(\"out_step1\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MIN_SAMPLES = 200\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5b346",
   "metadata": {},
   "source": [
    "### MVP for reading and checking for plausibolity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abe01e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === DATA DISCOVERY (Multi-Root) ===\n",
      "Sammelschienen erkannt: ['SHUW', 'Tarp', 'JUBO']\n",
      "Knoten gesamt: 11 | Kanten: 10 | Feeder: 5\n",
      " -- Verfügbare Messdateien (Nodes) --\n",
      "vorhanden: 6 | fehlend: 5\n",
      " -- Verfügbare Messdateien (Feeder) -- (virtuell aus Downstream-Summe)\n",
      "Hinweis: Feeder-Targets werden als Summe der Downstream-UW-Felder gebildet.\n",
      " === ZEITACHSE & FEATURE-SELEKTION JE FEEDER ===\n",
      "Feeder SHUW_SS_E24: Downstream-Features=3 | gemeinsame Samples=1343\n",
      "Feeder TARP_SS_TARP_E01: Downstream-Features=1 | gemeinsame Samples=1343\n",
      "Feeder JUBO_E03_JUBO: Downstream-Features=1 | gemeinsame Samples=1344\n",
      "Feeder JUBO_E02_JUBO: Downstream-Features=1 | gemeinsame Samples=1344\n",
      "Feeder JUBO_E01_JUBO: Downstream-Features=1 | gemeinsame Samples=1344\n",
      " === DIAGNOSTIK ===\n",
      "          feeder  n_samples  features       r2\n",
      "     SHUW_SS_E24       1343         3 0.999997\n",
      "TARP_SS_TARP_E01       1343         1 0.999999\n",
      "   JUBO_E03_JUBO       1344         1 0.999999\n",
      "   JUBO_E02_JUBO       1344         1 0.999999\n",
      "   JUBO_E01_JUBO       1344         1 0.999999\n",
      "→ gespeichert: out_step1\\diagnostics.csv\n",
      " === ERSTE SENSITIVITÄTEN (virtuell) ===\n",
      "                  Tarp_E01  JUBO_E01  SHUW_E24  JUBO_E03  JUBO_E02\n",
      "SHUW_SS_E24       0.999041  0.996863  0.997955  0.000000  0.000000\n",
      "TARP_SS_TARP_E01  0.999255  0.000000  0.000000  0.000000  0.000000\n",
      "JUBO_E03_JUBO     0.000000  0.000000  0.000000  0.999256  0.000000\n",
      "JUBO_E02_JUBO     0.000000  0.000000  0.000000  0.000000  0.999256\n",
      "JUBO_E01_JUBO     0.000000  0.999256  0.000000  0.000000  0.000000\n",
      "→ gespeichert: out_step1\\sensitivities.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Graph laden (Multi-Root)\n",
    "# ----------------------------\n",
    "\n",
    "def load_graph(path: Path):\n",
    "    def coerce_nodes_edges(data):\n",
    "        if isinstance(data, dict):\n",
    "            for k in (\"graph\", \"data\", \"payload\", \"content\"):\n",
    "                if k in data and isinstance(data[k], (dict, list)):\n",
    "                    data = data[k]\n",
    "                    break\n",
    "        if isinstance(data, list) and data and isinstance(data[0], dict) and \"data\" in data[0]:\n",
    "            nodes, edges = [], []\n",
    "            for el in data:\n",
    "                d = el.get(\"data\", {})\n",
    "                if \"source\" in d and \"target\" in d:\n",
    "                    edges.append({\n",
    "                        \"id\": d.get(\"id\"),\n",
    "                        \"source\": d.get(\"source\"),\n",
    "                        \"target\": d.get(\"target\"),\n",
    "                        \"label\": d.get(\"label\"),\n",
    "                        \"features\": d.get(\"features\", {})\n",
    "                    })\n",
    "                elif \"id\" in d:\n",
    "                    nodes.append({\n",
    "                        \"id\": d.get(\"id\"),\n",
    "                        \"label\": d.get(\"label\", d.get(\"id\")),\n",
    "                        \"type\": el.get(\"type\") or d.get(\"type\"),\n",
    "                        \"features\": d.get(\"features\", {}),\n",
    "                        \"position\": el.get(\"position\", {})\n",
    "                    })\n",
    "            return nodes, edges\n",
    "        if isinstance(data, dict) and \"nodes\" in data and (\"edges\" in data or \"links\" in data):\n",
    "            nodes = data[\"nodes\"]\n",
    "            edges = data.get(\"edges\", data.get(\"links\", []))\n",
    "            return nodes, edges\n",
    "        if isinstance(data, list) and data and isinstance(data[0], dict):\n",
    "            sample = data[0]\n",
    "            if (\"from\" in sample and \"to\" in sample) or (\"source\" in sample and \"target\" in sample):\n",
    "                edges = data\n",
    "                node_ids = {e.get(\"from\", e.get(\"source\")) for e in edges} | {e.get(\"to\", e.get(\"target\")) for e in edges}\n",
    "                nodes = [{\"id\": n} for n in sorted(node_ids)]\n",
    "                return nodes, edges\n",
    "        raise ValueError(\"Unbekanntes JSON-Format für Graph.\")\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    nodes_raw, edges_raw = coerce_nodes_edges(raw)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    edges_rows = []\n",
    "    for n in nodes_raw:\n",
    "        nid = n.get(\"id\")\n",
    "        if nid is None:\n",
    "            raise ValueError(\"Knoten ohne 'id'.\")\n",
    "        attrs = {k: v for k, v in n.items() if k != \"id\"}\n",
    "        G.add_node(str(nid), **attrs)\n",
    "    for e in edges_raw:\n",
    "        u = e.get(\"from\", e.get(\"source\"))\n",
    "        v = e.get(\"to\", e.get(\"target\"))\n",
    "        if not u or not v:\n",
    "            raise ValueError(\"Kante ohne 'from'/'to'.\")\n",
    "        eid = e.get(\"id\") or f\"{u}->{v}\"\n",
    "        G.add_edge(str(u), str(v), id=str(eid))\n",
    "        edges_rows.append({\"edge_id\": str(eid), \"from\": str(u), \"to\": str(v)})\n",
    "\n",
    "    # Multi-Root Orientierung: alle Sammelschienen gleichzeitig\n",
    "    roots = [n for n, a in G.nodes(data=True) if a.get(\"type\") == \"busbar\"]\n",
    "    if not roots:\n",
    "        raise ValueError(\"Keine Sammelschienen (type='busbar') im Graph gefunden.\")\n",
    "\n",
    "    depths = {}\n",
    "    for r in roots:\n",
    "        d = nx.single_source_shortest_path_length(G, r)\n",
    "        for k, v in d.items():\n",
    "            depths.setdefault(k, v)\n",
    "\n",
    "    oriented = []\n",
    "    for e in edges_rows:\n",
    "        u, v = e[\"from\"], e[\"to\"]\n",
    "        du, dv = depths.get(u, np.inf), depths.get(v, np.inf)\n",
    "        if G.nodes[u].get(\"type\") == \"busbar\":\n",
    "            frm, to = u, v\n",
    "        elif G.nodes[v].get(\"type\") == \"busbar\":\n",
    "            frm, to = v, u\n",
    "        else:\n",
    "            frm, to = (u, v) if du < dv else (v, u)\n",
    "        oriented.append({\"edge_id\": e[\"edge_id\"], \"from\": frm, \"to\": to})\n",
    "\n",
    "    oriented_df = pd.DataFrame(oriented)\n",
    "    return G, roots, oriented_df\n",
    "\n",
    "# ----------------------------\n",
    "# Hilfsfunktionen\n",
    "# ----------------------------\n",
    "\n",
    "def feeders_from_busbars(oriented_edges: pd.DataFrame, roots: list[str]) -> pd.DataFrame:\n",
    "    return oriented_edges.loc[oriented_edges[\"from\"].isin(roots), [\"edge_id\", \"from\", \"to\"]].reset_index(drop=True)\n",
    "\n",
    "def downstream_sets(oriented_edges: pd.DataFrame) -> dict[str, set]:\n",
    "    Di = nx.DiGraph()\n",
    "    for _, r in oriented_edges.iterrows():\n",
    "        Di.add_edge(r[\"from\"], r[\"to\"], edge_id=r[\"edge_id\"])\n",
    "    children = {}\n",
    "    for u, v in Di.edges():\n",
    "        children.setdefault(u, []).append(v)\n",
    "\n",
    "    def collect(start):\n",
    "        stack = [start]\n",
    "        seen = set()\n",
    "        while stack:\n",
    "            x = stack.pop()\n",
    "            if x in seen:\n",
    "                continue\n",
    "            seen.add(x)\n",
    "            stack.extend(children.get(x, []))\n",
    "        return seen\n",
    "\n",
    "    ds = {}\n",
    "    for _, r in oriented_edges.iterrows():\n",
    "        ds[r[\"edge_id\"]] = collect(r[\"to\"])\n",
    "    return ds\n",
    "\n",
    "def load_series(meas_dir: Path, series_id: str) -> pd.Series | None:\n",
    "    f = meas_dir / f\"{series_id}.csv\"\n",
    "    if not f.exists():\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(f)\n",
    "    if TS_COL not in df.columns or VAL_COL not in df.columns:\n",
    "        raise ValueError(f\"{f} muss Spalten '{TS_COL}' und '{VAL_COL}' enthalten.\")\n",
    "\n",
    "    # --- Hauptunterschied: Zeitzone vereinheitlichen ---\n",
    "    # 1) immer als UTC parsen (funktioniert auch bei \"+00:00\"-Timestamps)\n",
    "    df[TS_COL] = pd.to_datetime(df[TS_COL], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # 2) Spalte als Float; Index ist UTC-naiv (Zeitzonen entfernt)\n",
    "    s = (\n",
    "        df.dropna(subset=[TS_COL, VAL_COL])\n",
    "          .set_index(TS_COL)[VAL_COL]\n",
    "          .astype(\"float64\")\n",
    "          .sort_index()\n",
    "          .tz_convert(None)   # entfernt Zeitzoneninfo (alle naiv, aber in UTC-Zeit)\n",
    "    )\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def fit_ridge(y: pd.Series, X: pd.DataFrame, alpha: float) -> tuple[pd.Series, float]:\n",
    "    idx = y.index.intersection(X.index)\n",
    "    y, X = y.loc[idx], X.loc[idx]\n",
    "    if len(idx) < MIN_SAMPLES:\n",
    "        return pd.Series(dtype=float), float(\"nan\")\n",
    "    Xm, Xs = X.mean(0), X.std(0).replace(0, 1.0)\n",
    "    ym, ys = y.mean(), y.std()\n",
    "    Xz, yz = (X - Xm) / Xs, (y - ym) / (ys if ys != 0 else 1.0)\n",
    "    mdl = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    mdl.fit(Xz, yz)\n",
    "    beta = (ys * mdl.coef_) / Xs.values\n",
    "    r2 = mdl.score(Xz, yz) if ys != 0 else 1.0\n",
    "    return pd.Series(beta, index=X.columns), float(r2)\n",
    "\n",
    "# ----------------------------\n",
    "# Virtuelles Feeder-Target aus UW-Feld-Zeitreihen bilden\n",
    "# ----------------------------\n",
    "\n",
    "def build_virtual_feeder_target(fid: str, ds_map: dict[str,set], meas_dir: Path, G: nx.Graph,\n",
    "                                 only_uw_fields: bool = True) -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    Bildet ein virtuelles Target für den Feeder fid als SUMME der Downstream-UW-Feld-Zeitreihen.\n",
    "    - Nur Knoten mit vorhandener CSV werden berücksichtigt.\n",
    "    - Wenn only_uw_fields=True: es werden nur Nodes mit type=='uw_field' summiert.\n",
    "    Vorzeichen: +1 pro Downstream-Knoten (Verbraucherzähpfeilrichtung).\n",
    "    \"\"\"\n",
    "    nodes_ds = list(ds_map[fid])\n",
    "    if only_uw_fields:\n",
    "        nodes_ds = [n for n in nodes_ds if str(G.nodes[n].get(\"type\")) == \"uw_field\"]\n",
    "    parts = []\n",
    "    for n in nodes_ds:\n",
    "        s = load_series(meas_dir, n)\n",
    "        if s is not None:\n",
    "            parts.append(s.rename(n))\n",
    "    if not parts:\n",
    "        return None\n",
    "    # Zeitlich schneiden (inner join) und aufsummieren\n",
    "    X = pd.concat(parts, axis=1, join=\"inner\").dropna()\n",
    "    if X.empty:\n",
    "        return None\n",
    "    y = X.sum(axis=1)\n",
    "    y.name = fid\n",
    "    return y\n",
    "\n",
    "# ----------------------------\n",
    "# Main Pipeline\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" === DATA DISCOVERY (Multi-Root) ===\")\n",
    "    G, ROOTS, EDGES = load_graph(GRAPH_JSON)\n",
    "    FEEDERS = feeders_from_busbars(EDGES, ROOTS)\n",
    "    DS = downstream_sets(EDGES)\n",
    "    node_ids = sorted(G.nodes())\n",
    "    feeder_ids = FEEDERS[\"edge_id\"].tolist()\n",
    "\n",
    "    print(f\"Sammelschienen erkannt: {ROOTS}\")\n",
    "    print(f\"Knoten gesamt: {len(node_ids)} | Kanten: {EDGES.shape[0]} | Feeder: {len(feeder_ids)}\")\n",
    "\n",
    "    print(\" -- Verfügbare Messdateien (Nodes) --\")\n",
    "    have_nodes = [n for n in node_ids if (MEAS_DIR / f\"{n}.csv\").exists()]\n",
    "    missing_nodes = [n for n in node_ids if n not in have_nodes]\n",
    "    print(f\"vorhanden: {len(have_nodes)} | fehlend: {len(missing_nodes)}\")\n",
    "\n",
    "    print(\" -- Verfügbare Messdateien (Feeder) -- (virtuell aus Downstream-Summe)\")\n",
    "    print(\"Hinweis: Feeder-Targets werden als Summe der Downstream-UW-Felder gebildet.\")\n",
    "\n",
    "    print(\" === ZEITACHSE & FEATURE-SELEKTION JE FEEDER ===\")\n",
    "    rows_diag, sensitivities = [], []\n",
    "    for fid in feeder_ids:\n",
    "        # 1) Virtuelles Target (Summe der Downstream-UW-Felder)\n",
    "        y = build_virtual_feeder_target(fid, DS, MEAS_DIR, G, only_uw_fields=True)\n",
    "        if y is None:\n",
    "            print(f\"Feeder {fid}: kein virtuelles Target (keine Downstream-UW-Feld-Zeitreihen) → übersprungen\")\n",
    "            rows_diag.append({\"feeder\": fid, \"n_samples\": 0, \"features\": 0, \"r2\": np.nan})\n",
    "            continue\n",
    "\n",
    "        # 2) Prädiktoren = einzelne Downstream-UW-Feld-Zeitreihen (die im Target stecken)\n",
    "        ds_nodes = [n for n in DS[fid] if n in have_nodes and str(G.nodes[n].get(\"type\")) == \"uw_field\"]\n",
    "        X_parts = [load_series(MEAS_DIR, n).rename(n) for n in ds_nodes if load_series(MEAS_DIR, n) is not None]\n",
    "        if not X_parts:\n",
    "            print(f\"Feeder {fid}: keine Downstream-Knoten mit Messdaten → übersprungen\")\n",
    "            rows_diag.append({\"feeder\": fid, \"n_samples\": 0, \"features\": 0, \"r2\": np.nan})\n",
    "            continue\n",
    "        X = pd.concat(X_parts, axis=1, join=\"inner\").dropna()\n",
    "        # 3) Schnittmenge der Zeitachsen zwischen y und X\n",
    "        idx = y.index.intersection(X.index)\n",
    "        n_samples = len(idx)\n",
    "        print(f\"Feeder {fid}: Downstream-Features={len(X.columns)} | gemeinsame Samples={n_samples}\")\n",
    "        if n_samples < MIN_SAMPLES:\n",
    "            print(f\"  → zu wenig Samples (<{MIN_SAMPLES}); Regression wird übersprungen\")\n",
    "            rows_diag.append({\"feeder\": fid, \"n_samples\": n_samples, \"features\": len(X.columns), \"r2\": np.nan})\n",
    "            continue\n",
    "\n",
    "        # 4) Regression (liefert Sensitivitäten ~1.0, dient aber als Konsistenz-Check und für spätere Erweiterungen)\n",
    "        beta, r2 = fit_ridge(y.loc[idx], X.loc[idx], ALPHA_RIDGE)\n",
    "        rows_diag.append({\"feeder\": fid, \"n_samples\": n_samples, \"features\": len(X.columns), \"r2\": r2})\n",
    "        sensitivities.append(beta.rename(fid))\n",
    "\n",
    "    diag_df = pd.DataFrame(rows_diag)\n",
    "    diag_path = OUT_DIR / \"diagnostics.csv\"\n",
    "    diag_df.to_csv(diag_path, index=False)\n",
    "    print(\" === DIAGNOSTIK ===\")\n",
    "    print(diag_df.to_string(index=False))\n",
    "    print(f\"→ gespeichert: {diag_path}\")\n",
    "\n",
    "    if sensitivities:\n",
    "        S = pd.DataFrame(sensitivities).fillna(0.0)\n",
    "        S_path = OUT_DIR / \"sensitivities.csv\"\n",
    "        S.to_csv(S_path)\n",
    "        print(\" === ERSTE SENSITIVITÄTEN (virtuell) ===\")\n",
    "        print(S.head().to_string())\n",
    "        print(f\"→ gespeichert: {S_path}\")\n",
    "    else:\n",
    "        print(\" Keine ausreichenden Daten für eine Sensitivitätstabelle. Siehe diagnostics.csv für Details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da8ddf",
   "metadata": {},
   "source": [
    "## Creating a first actual sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3e9472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene UW-Felder: ['JUBO_E01', 'JUBO_E02', 'JUBO_E03', 'SHUW_E24', 'Tarp_E01']\n",
      "Gemeinsame Zeitpunkte: 1343 | UW-Felder im Datensatz: 5\n",
      "\n",
      "=== SENSITIVITÄTSMATRIX UW↔UW ===\n",
      "          JUBO_E01  JUBO_E02  JUBO_E03  SHUW_E24  Tarp_E01\n",
      "JUBO_E01       NaN    -0.790    -0.356    -0.001     0.000\n",
      "JUBO_E02    -1.261       NaN    -0.448     0.001    -0.001\n",
      "JUBO_E03    -2.795    -2.205       NaN    -0.002     0.000\n",
      "SHUW_E24    -0.345     0.507    -0.158       NaN     0.001\n",
      "Tarp_E01     0.311    -0.413     0.045     0.001       NaN\n",
      "\n",
      "R² je Regressionsziel:\n",
      "JUBO_E01: 1.0000\n",
      "JUBO_E02: 1.0000\n",
      "JUBO_E03: 1.0000\n",
      "SHUW_E24: 0.5835\n",
      "Tarp_E01: 0.4756\n",
      "\n",
      "→ gespeichert: out_step2_sensitivity_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from pathlib import Path\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "GRAPH_JSON = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\graph\\graph_with_jubo_e01.json\")\n",
    "MEAS_DIR   = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\handling_graph\\out\\nodes\")\n",
    "TS_COL     = \"timestamp\"\n",
    "VAL_COL    = \"P_Datapoint_ID\"\n",
    "ALPHA_RIDGE = 0.5\n",
    "MIN_SAMPLES = 200\n",
    "\n",
    "# --- 1. UW-Felder extrahieren ---\n",
    "def load_uw_fields(graph_path: Path):\n",
    "    data = json.loads(graph_path.read_text(encoding=\"utf-8\"))\n",
    "    uw_nodes = []\n",
    "    for el in data:\n",
    "        d = el.get(\"data\", {})\n",
    "        if d.get(\"type\") == \"uw_field\":\n",
    "            uw_nodes.append(d[\"id\"])\n",
    "    return sorted(uw_nodes)\n",
    "\n",
    "uw_nodes = load_uw_fields(GRAPH_JSON)\n",
    "print(f\"Gefundene UW-Felder: {uw_nodes}\")\n",
    "\n",
    "# --- 2. Messwerte laden und auf gemeinsame Zeitachse bringen ---\n",
    "def load_series(name):\n",
    "    df = pd.read_csv(MEAS_DIR / f\"{name}.csv\")\n",
    "    df[TS_COL] = pd.to_datetime(df[TS_COL], errors=\"coerce\", utc=True)\n",
    "    s = df.dropna(subset=[TS_COL, VAL_COL]).set_index(TS_COL)[VAL_COL].astype(float)\n",
    "    return s.tz_convert(None).sort_index()\n",
    "\n",
    "series = {n: load_series(n) for n in uw_nodes if (MEAS_DIR / f\"{n}.csv\").exists()}\n",
    "\n",
    "df_all = pd.concat(series.values(), axis=1, join=\"inner\")\n",
    "df_all.columns = list(series.keys())\n",
    "print(f\"Gemeinsame Zeitpunkte: {len(df_all)} | UW-Felder im Datensatz: {df_all.shape[1]}\")\n",
    "\n",
    "if len(df_all) < MIN_SAMPLES:\n",
    "    raise RuntimeError(\"Zu wenige gemeinsame Zeitpunkte für robuste Regression.\")\n",
    "\n",
    "# --- 3. Ridge-Regression je UW-Feld ---\n",
    "S = pd.DataFrame(index=df_all.columns, columns=df_all.columns, dtype=float)\n",
    "r2_scores = {}\n",
    "\n",
    "for target in df_all.columns:\n",
    "    y = df_all[target]\n",
    "    X = df_all.drop(columns=[target])\n",
    "    X = (X - X.mean()) / X.std().replace(0, 1)\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    mdl = Ridge(alpha=ALPHA_RIDGE)\n",
    "    mdl.fit(X, y)\n",
    "    beta = pd.Series(mdl.coef_, index=X.columns)\n",
    "    S.loc[target, X.columns] = beta\n",
    "    r2_scores[target] = mdl.score(X, y)\n",
    "\n",
    "print(\"\\n=== SENSITIVITÄTSMATRIX UW↔UW ===\")\n",
    "print(S.round(3))\n",
    "print(\"\\nR² je Regressionsziel:\")\n",
    "for k,v in r2_scores.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "S.to_csv(\"out_step2_sensitivity_matrix.csv\")\n",
    "print(\"\\n→ gespeichert: out_step2_sensitivity_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394444a",
   "metadata": {},
   "source": [
    "# BOLS fehlt hier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da09382",
   "metadata": {},
   "source": [
    "# Weitere Steps (Vorläufig:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedd8d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAR mit p=1\n",
      "\n",
      "Aggregierte Einwirkungsmatrix (nicht-orthogonalisierte IRFs):\n",
      "          JUBO_E01  JUBO_E02  JUBO_E03  SHUW_E24  Tarp_E01\n",
      "JUBO_E01     1.027    -0.052     0.042    -0.142     0.178\n",
      "JUBO_E02    -0.006     1.018    -0.023     0.105    -0.134\n",
      "JUBO_E03    -0.055     0.090     0.946     0.126    -0.154\n",
      "SHUW_E24     0.002     0.024    -0.057     1.075    -0.100\n",
      "Tarp_E01     0.006     0.017    -0.050    -0.166     1.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# Voraussetzung: df_all existiert bereits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# 1) Differenzieren + Standardisieren\n",
    "df_d = df_all.diff().dropna()\n",
    "# sehr kleine Varianzen droppen\n",
    "stds = df_d.std()\n",
    "df_d = df_d.drop(columns=stds[stds < 1e-9].index)\n",
    "df_z = (df_d - df_d.mean()) / df_d.std().replace(0, 1)\n",
    "\n",
    "# 2) Lagwahl kleingehalten (robuster)\n",
    "maxlags = 1\n",
    "try:\n",
    "    sel = VAR(df_z).select_order(maxlags=maxlags)\n",
    "    p = int(np.clip(sel.aic, 1, 2))   # p∈{1,2}\n",
    "except Exception:\n",
    "    p = 1\n",
    "print(f\"VAR mit p={p}\")\n",
    "var = VAR(df_z).fit(maxlags=p)\n",
    "\n",
    "# 3) IRFs OHNE Cholesky (direkt aus A_l), d.h. nicht-orthogonalisiert\n",
    "#    y_t = sum_{l=1..p} A_l y_{t-l} + u_t, A_l ∈ R^{k×k}\n",
    "A_list = [var.coefs[l] for l in range(p)]  # shape: (p, k, k)\n",
    "\n",
    "def irfs_from_A(A_list, H: int):\n",
    "    \"\"\"Nicht-orthogonalisierte IRFs Ψ_0..Ψ_H via Rekursion: Ψ_h = sum_l A_l Ψ_{h-l}.\"\"\"\n",
    "    k = A_list[0].shape[0]\n",
    "    Psis = [np.eye(k)]\n",
    "    for h in range(1, H+1):\n",
    "        acc = np.zeros((k, k))\n",
    "        for l, A_l in enumerate(A_list, start=1):\n",
    "            if h - l >= 0:\n",
    "                acc += A_l @ Psis[h - l]\n",
    "        Psis.append(acc)\n",
    "    return Psis  # Liste: Ψ_0, Ψ_1, ..., Ψ_H (je k×k)\n",
    "\n",
    "H = 1\n",
    "Psis = irfs_from_A(A_list, H=H)\n",
    "M_total = np.sum(Psis, axis=0)  # aggregierte Einwirkungsmatrix bis Horizont H\n",
    "\n",
    "uw_order = list(df_z.columns)\n",
    "M_df = pd.DataFrame(M_total, index=uw_order, columns=uw_order)\n",
    "print(\"\\nAggregierte Einwirkungsmatrix (nicht-orthogonalisierte IRFs):\")\n",
    "print(M_df.round(3))\n",
    "\n",
    "# Hilfsfunktion: Reaktionsvektor für Impuls in UW A (Δp über alle UWs pro 1 MW in A)\n",
    "def reaction_vector(uw_name, delta=1.0):\n",
    "    a_idx = uw_order.index(uw_name)\n",
    "    v = M_total[:, a_idx] * delta\n",
    "    return pd.Series(v, index=uw_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce11673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTDF für 1 Komponente(n) aufgebaut.\n",
      "\n",
      "Top-10 Fluss-Änderungen bei +1 MW in JUBO_E01:\n",
      "e:SHUW--SHUW_E24      -0.9755\n",
      "e:SHUW_E24--JUBO_A5   -0.9730\n",
      "e:JUBO_E01--JUBO_A5    0.9668\n",
      "e:JUBO_E01--JUBO       0.0605\n",
      "e:JUBO_E03--JUBO      -0.0550\n",
      "e:JUBO_A5--BOLS_A5    -0.0062\n",
      "e:Tarp_E01--BOLS_A5    0.0062\n",
      "e:JUBO_E02--JUBO      -0.0055\n",
      "e:Tarp--Tarp_E01      -0.0000\n",
      "e:BOLS_A5--BOLS_E42    0.0000\n",
      "dtype: float64\n",
      "\n",
      "Historische Flüsse: 1342 Zeitpunkte, 10 Leitungen\n",
      "Aktive Leitungen für Bandberechnung: 7 / 10\n",
      "\n",
      "=== Deterministische Leistungsbänder (robust) ===\n",
      "          DeltaP_plus_MW  DeltaP_minus_MW\n",
      "JUBO_E01          15.753           15.753\n",
      "JUBO_E02           9.555            9.555\n",
      "JUBO_E03           5.262            5.262\n",
      "SHUW_E24          39.508           39.508\n",
      "Tarp_E01          12.951           12.951\n"
     ]
    }
   ],
   "source": [
    "# === PTDF (x=1), Impuls->Flüsse, historische Flüsse & deterministische Bänder ===\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- 1) PTDF je verbundener Komponente (Slack = Busbar falls vorhanden) --------\n",
    "def build_ptdf_components(graph_json_path: Path):\n",
    "    elems = json.loads(Path(graph_json_path).read_text(encoding=\"utf-8\"))\n",
    "    G = nx.Graph()\n",
    "    for el in elems:\n",
    "        d = el.get(\"data\", {})\n",
    "        if 'source' in d and 'target' in d:\n",
    "            G.add_edge(d['source'], d['target'], x=1.0)  # Einheitsreaktanz\n",
    "        elif d.get(\"id\") and d.get(\"type\") in {\"busbar\", \"uw_field\"}:\n",
    "            G.add_node(d[\"id\"], ntype=d.get(\"type\"))\n",
    "\n",
    "    comps = []\n",
    "    for nodes in nx.connected_components(G):\n",
    "        Gc = G.subgraph(nodes).copy()\n",
    "        # Knoten & Indizes\n",
    "        nodes = list(Gc.nodes())\n",
    "        idx = {n:i for i,n in enumerate(nodes)}\n",
    "        edges = list(Gc.edges())\n",
    "        m, n = len(edges), len(nodes)\n",
    "        if n == 0 or m == 0:\n",
    "            continue\n",
    "\n",
    "        # Inzidenz A (m x n); Orientierung beliebig, aber konsistent\n",
    "        A = np.zeros((m, n))\n",
    "        for e_i,(u,v) in enumerate(edges):\n",
    "            A[e_i, idx[u]] = +1.0\n",
    "            A[e_i, idx[v]] = -1.0\n",
    "\n",
    "        # Slack: bevorzugt Busbar, sonst 1. Knoten\n",
    "        busbars = [n for n in nodes if Gc.nodes[n].get('ntype') == 'busbar']\n",
    "        slack_node = busbars[0] if busbars else nodes[0]\n",
    "        slack_idx = idx[slack_node]\n",
    "\n",
    "        keep = [i for i in range(n) if i != slack_idx]\n",
    "        B = A.T @ A  # x=1 => Laplacian\n",
    "        B_rr = B[np.ix_(keep, keep)]\n",
    "        # Pseudoinverse ist robust, auch wenn B_rr nicht vollen Rang hat\n",
    "        B_rr_inv = np.linalg.pinv(B_rr)\n",
    "\n",
    "        # PTDF: f = A * theta; theta_r = B_rr^{-1} * p_r  -> f = A[:,keep] * B_rr^{-1} * p_r\n",
    "        H = A[:, keep] @ B_rr_inv  # (m x (n-1))\n",
    "\n",
    "        comps.append({\n",
    "            \"graph\": Gc,\n",
    "            \"nodes\": nodes,\n",
    "            \"edges\": edges,\n",
    "            \"A\": A,\n",
    "            \"PTDF\": H,\n",
    "            \"keep\": keep,\n",
    "            \"slack_idx\": slack_idx,\n",
    "            \"slack_node\": slack_node\n",
    "        })\n",
    "    return comps\n",
    "\n",
    "ptdf_components = build_ptdf_components(GRAPH_JSON)\n",
    "print(f\"PTDF für {len(ptdf_components)} Komponente(n) aufgebaut.\")\n",
    "if not ptdf_components:\n",
    "    raise RuntimeError(\"Keine PTDF-Komponente gefunden (Graph leer?).\")\n",
    "\n",
    "# -------- 2) Reaktionsvektor aus M_total (Impuls in A -> Δp aller UWs) --------\n",
    "def reaction_vector(uw_name: str, delta: float = 1.0) -> pd.Series:\n",
    "    a = uw_order.index(uw_name)\n",
    "    v = M_total[:, a] * delta\n",
    "    return pd.Series(v, index=uw_order)\n",
    "\n",
    "# -------- 3) Impuls -> Linienflussänderungen (über alle Komponenten) --------\n",
    "def flows_from_impulse(uw_name: str, delta: float = 1.0) -> pd.Series:\n",
    "    v = reaction_vector(uw_name, delta=delta)  # Δp pro UW\n",
    "    all_flows = []\n",
    "    for comp in ptdf_components:\n",
    "        nodes = comp[\"nodes\"]; keep = comp[\"keep\"]; Hm = comp[\"PTDF\"]; edges = comp[\"edges\"]; sidx = comp[\"slack_idx\"]\n",
    "        # Δp nur auf Knoten der Komponente abbilden\n",
    "        p_full = np.zeros(len(nodes))\n",
    "        for i, n in enumerate(nodes):\n",
    "            if n in v.index:\n",
    "                p_full[i] = v[n]\n",
    "        # Slack balancieren (Summe null)\n",
    "        p_full[sidx] = - (p_full.sum() - p_full[sidx])\n",
    "        pr = p_full[keep]\n",
    "        f = Hm @ pr  # (m,)\n",
    "        edge_names = [f\"e:{u}--{v}\" for (u,v) in edges]\n",
    "        all_flows.append(pd.Series(f, index=edge_names))\n",
    "    if all_flows:\n",
    "        out = pd.concat(all_flows)\n",
    "        # sinnvolle Sortierung nach Betrag\n",
    "        return out.reindex(out.abs().sort_values(ascending=False).index)\n",
    "    return pd.Series(dtype=float)\n",
    "\n",
    "# Quick sanity: Top-Kanten für einen Beispielimpuls\n",
    "try:\n",
    "    print(\"\\nTop-10 Fluss-Änderungen bei +1 MW in JUBO_E01:\")\n",
    "    print(flows_from_impulse(\"JUBO_E01\", 1.0).head(10).round(4))\n",
    "except Exception as e:\n",
    "    print(\"Hinweis:\", e)\n",
    "\n",
    "# -------- 4) Historische Linienflüsse aus ΔP(t) für Quantil-Grenzen --------\n",
    "def compute_historical_flows(df_delta: pd.DataFrame) -> pd.DataFrame:\n",
    "    all_frames = []\n",
    "    for comp in ptdf_components:\n",
    "        nodes = comp[\"nodes\"]; keep = comp[\"keep\"]; Hm = comp[\"PTDF\"]; edges = comp[\"edges\"]; sidx = comp[\"slack_idx\"]\n",
    "        X = df_delta.reindex(columns=nodes, fill_value=0.0).copy()\n",
    "        P = X.values  # (T x n)\n",
    "        # Slack-Spalte balancieren\n",
    "        P[:, sidx] = - (P.sum(axis=1) - P[:, sidx])\n",
    "        P_r = P[:, keep]                   # (T x (n-1))\n",
    "        F = (P_r @ Hm.T)                   # (T x m) => f = H * p_r  => F = P_r * H^T\n",
    "        cols = [f\"e:{u}--{v}\" for (u,v) in edges]\n",
    "        all_frames.append(pd.DataFrame(F, index=X.index, columns=cols))\n",
    "    return pd.concat(all_frames, axis=1)\n",
    "\n",
    "df_delta = df_all.diff().dropna()\n",
    "F_hist = compute_historical_flows(df_delta)\n",
    "print(f\"\\nHistorische Flüsse: {F_hist.shape[0]} Zeitpunkte, {F_hist.shape[1]} Leitungen\")\n",
    "\n",
    "# Grenzwerte konservativ über hohes Quantil\n",
    "alpha_quant = 0.995\n",
    "Fmax = F_hist.abs().quantile(alpha_quant, axis=0)   # Serie je Leitung\n",
    "# --- ab hier den alten Band-Teil ersetzen ---\n",
    "\n",
    "# Grenzwerte-Basis (Median bleibt)\n",
    "f0 = F_hist.median(axis=0)\n",
    "\n",
    "# 1) Robuste Limits & aktive Leitungen\n",
    "k_sigma = 4.0\n",
    "eps_cap = 1e-3  # ggf. erhöhen (z.B. 0.01)\n",
    "Fstd = F_hist.std(axis=0)\n",
    "\n",
    "Fmax_robust = pd.concat([\n",
    "    F_hist.abs().quantile(0.995, axis=0),\n",
    "    k_sigma * Fstd,\n",
    "    pd.Series(eps_cap, index=F_hist.columns)\n",
    "], axis=1).max(axis=1)\n",
    "\n",
    "active_edges = Fmax_robust[Fmax_robust > eps_cap].index.tolist()\n",
    "print(f\"Aktive Leitungen für Bandberechnung: {len(active_edges)} / {F_hist.shape[1]}\")\n",
    "\n",
    "# 2) Bandfunktion (nur aktive Kanten, robustes Limit)\n",
    "def deterministic_band_for_UW_robust(uw_name: str) -> tuple[float, float]:\n",
    "    f_unit = flows_from_impulse(uw_name, 1.0)\n",
    "    if f_unit.empty:\n",
    "        return 0.0, 0.0\n",
    "    f_unit = f_unit.reindex(active_edges).fillna(0.0)\n",
    "    if (f_unit.abs() <= 1e-12).all():\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    f0_act   = f0.reindex(active_edges).fillna(0.0)\n",
    "    Fcap_act = Fmax_robust.reindex(active_edges).fillna(eps_cap)\n",
    "\n",
    "    ratios = []\n",
    "    for e in active_edges:\n",
    "        denom = abs(f_unit[e])\n",
    "        if denom <= 1e-12:\n",
    "            continue\n",
    "        cap = max(Fcap_act[e] - abs(f0_act[e]), 0.0)\n",
    "        ratios.append(cap / denom)\n",
    "\n",
    "    if not ratios:\n",
    "        return 0.0, 0.0\n",
    "    band = float(np.min(ratios))\n",
    "    return band, band  # symmetrisch\n",
    "\n",
    "bands_rob = {uw: deterministic_band_for_UW_robust(uw) for uw in uw_order}\n",
    "bands_rob_df = pd.DataFrame(bands_rob, index=[\"DeltaP_plus_MW\", \"DeltaP_minus_MW\"]).T\n",
    "print(\"\\n=== Deterministische Leistungsbänder (robust) ===\")\n",
    "print(bands_rob_df.round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e01d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stochastische Leistungsbänder (α = 5%) ===\n",
      "          DeltaP_plus_MW  DeltaP_minus_MW\n",
      "JUBO_E01           9.190            9.190\n",
      "JUBO_E02           5.758            5.758\n",
      "JUBO_E03           3.298            3.298\n",
      "SHUW_E24          24.761           24.761\n",
      "Tarp_E01           7.556            7.556\n",
      "\n",
      "=== Stochastische Leistungsbänder (α = 1%) ===\n",
      "          DeltaP_plus_MW  DeltaP_minus_MW\n",
      "JUBO_E01           6.473            6.473\n",
      "JUBO_E02           4.186            4.186\n",
      "JUBO_E03           2.485            2.485\n",
      "SHUW_E24          17.863           17.863\n",
      "Tarp_E01           5.322            5.322\n",
      "\n",
      "Bindende Kanten je UW (innerhalb 1% des Minimums):\n",
      "- JUBO_E01: {'alpha=5%': ['e:SHUW_E24--JUBO_A5'], 'alpha=1%': ['e:SHUW_E24--JUBO_A5']}\n",
      "- JUBO_E02: {'alpha=5%': ['e:JUBO_E02--JUBO'], 'alpha=1%': ['e:JUBO_E02--JUBO']}\n",
      "- JUBO_E03: {'alpha=5%': ['e:JUBO_E03--JUBO'], 'alpha=1%': ['e:JUBO_E03--JUBO']}\n",
      "- SHUW_E24: {'alpha=5%': ['e:JUBO_E03--JUBO'], 'alpha=1%': ['e:SHUW--SHUW_E24']}\n",
      "- Tarp_E01: {'alpha=5%': ['e:Tarp_E01--BOLS_A5', 'e:JUBO_A5--BOLS_A5'], 'alpha=1%': ['e:Tarp_E01--BOLS_A5', 'e:JUBO_A5--BOLS_A5']}\n"
     ]
    }
   ],
   "source": [
    "# === Stochastische Bänder (Chance-Constraints) ==================================\n",
    "from math import isfinite\n",
    "\n",
    "# 1) Unsicherheitsmaß pro Leitung\n",
    "Fstd = F_hist.std(axis=0).reindex(active_edges).fillna(0.0)\n",
    "\n",
    "def stochastic_band_for_UW(uw_name: str, alpha: float) -> tuple[float, float, list]:\n",
    "    \"\"\"Chance-Constraint-Band: zieht z*σ_e von der Kapazität je Kante ab.\n",
    "       Rückgabe: (Band+, Band-, list(bindende_kanten))\"\"\"\n",
    "    z = {0.05: 1.645, 0.01: 2.326}.get(alpha, None)\n",
    "    if z is None:\n",
    "        raise ValueError(\"alpha nur 0.05 oder 0.01 im Beispiel.\")\n",
    "    f_unit = flows_from_impulse(uw_name, 1.0).reindex(active_edges).fillna(0.0)\n",
    "    if (f_unit.abs() <= 1e-12).all():\n",
    "        return 0.0, 0.0, []\n",
    "\n",
    "    f0_act   = f0.reindex(active_edges).fillna(0.0)\n",
    "    Fcap_act = Fmax_robust.reindex(active_edges).fillna(0.0)\n",
    "    Fsig_act = Fstd.reindex(active_edges).fillna(0.0)\n",
    "\n",
    "    ratios = []\n",
    "    binders = []\n",
    "    for e in active_edges:\n",
    "        denom = abs(f_unit[e])\n",
    "        if denom <= 1e-12:\n",
    "            continue\n",
    "        # effektives Limit unter Unsicherheit\n",
    "        cap_eff = Fcap_act[e] - z * Fsig_act[e] - abs(f0_act[e])\n",
    "        cap_eff = max(cap_eff, 0.0)\n",
    "        val = cap_eff / denom if denom > 0 else np.inf\n",
    "        if isfinite(val):\n",
    "            ratios.append((val, e))\n",
    "\n",
    "    if not ratios:\n",
    "        return 0.0, 0.0, []\n",
    "\n",
    "    band = float(min(ratios, key=lambda t: t[0])[0])\n",
    "    # bindende kanten innerhalb 1% um das Minimum\n",
    "    minv = band\n",
    "    binders = [e for v,e in ratios if v <= 1.01*minv]\n",
    "    return band, band, binders\n",
    "\n",
    "# 2) Bänder für alpha=5% und 1% berechnen\n",
    "results_5 = {}\n",
    "results_1 = {}\n",
    "binders_map = {}\n",
    "for uw in uw_order:\n",
    "    b5p, b5m, bind5 = stochastic_band_for_UW(uw, alpha=0.05)\n",
    "    b1p, b1m, bind1 = stochastic_band_for_UW(uw, alpha=0.01)\n",
    "    results_5[uw] = (b5p, b5m)\n",
    "    results_1[uw] = (b1p, b1m)\n",
    "    binders_map[uw] = {\"alpha=5%\": bind5, \"alpha=1%\": bind1}\n",
    "\n",
    "bands_5_df = pd.DataFrame(results_5, index=[\"DeltaP_plus_MW\", \"DeltaP_minus_MW\"]).T\n",
    "bands_1_df = pd.DataFrame(results_1, index=[\"DeltaP_plus_MW\", \"DeltaP_minus_MW\"]).T\n",
    "\n",
    "print(\"\\n=== Stochastische Leistungsbänder (α = 5%) ===\")\n",
    "print(bands_5_df.round(3))\n",
    "print(\"\\n=== Stochastische Leistungsbänder (α = 1%) ===\")\n",
    "print(bands_1_df.round(3))\n",
    "\n",
    "# 3) Zeig, welche Kanten limitieren (Binder)\n",
    "print(\"\\nBindende Kanten je UW (innerhalb 1% des Minimums):\")\n",
    "for uw, m in binders_map.items():\n",
    "    print(f\"- {uw}: {m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09e855b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity (1 MW in JUBO_E01):\n",
      "e:SHUW--SHUW_E24      -1.0158\n",
      "e:SHUW_E24--JUBO_A5   -1.0088\n",
      "e:JUBO_E01--JUBO_A5    1.0000\n",
      "e:JUBO_E01--JUBO       0.0273\n",
      "e:JUBO_E03--JUBO      -0.0226\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Einheiten-Korrektur: z-Score <-> MW ===\n",
    "# Voraussetzung: df_d = df_all.diff().dropna(); df_z = (df_d - mean)/std\n",
    "sigma = df_d.std().replace(0, 1)  # Std-Abweichungen der ΔP in MW pro UW (Index = uw_order)\n",
    "\n",
    "def reaction_vector_mw(uw_name: str, delta_mw: float = 1.0) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Reaktionsvektor in MW für einen Impuls von delta_mw (MW) in UW 'uw_name'.\n",
    "    M_total ist in z-Einheiten, daher:\n",
    "      Impuls in z:   delta_z_source = delta_mw / sigma[A]\n",
    "      Reaktion in z: M_total[:,A] * delta_z_source\n",
    "      zurück nach MW: multiply elementweise mit sigma[i]\n",
    "    \"\"\"\n",
    "    a = uw_order.index(uw_name)\n",
    "    delta_z_source = delta_mw / float(sigma.iloc[a])\n",
    "    resp_z = M_total[:, a] * delta_z_source  # z-Einheiten an allen UWs\n",
    "    resp_mw = pd.Series(resp_z, index=uw_order) * sigma  # zurück in MW\n",
    "    return resp_mw\n",
    "\n",
    "# flows_from_impulse soll MW liefern -> passe die Funktion an:\n",
    "def flows_from_impulse(uw_name: str, delta_mw: float = 1.0) -> pd.Series:\n",
    "    v = reaction_vector_mw(uw_name, delta_mw=delta_mw)  # jetzt MW\n",
    "    all_flows = []\n",
    "    for comp in ptdf_components:\n",
    "        nodes = comp[\"nodes\"]; keep = comp[\"keep\"]; Hm = comp[\"PTDF\"]; edges = comp[\"edges\"]; sidx = comp[\"slack_idx\"]\n",
    "        p_full = np.zeros(len(nodes))\n",
    "        for i, n in enumerate(nodes):\n",
    "            if n in v.index:\n",
    "                p_full[i] = v[n]\n",
    "        # Slack-Ausgleich innerhalb der Komponente\n",
    "        p_full[sidx] = - (p_full.sum() - p_full[sidx])\n",
    "        pr = p_full[keep]\n",
    "        f = Hm @ pr  # (m,)\n",
    "        edge_names = [f\"e:{u}--{v}\" for (u,v) in edges]\n",
    "        all_flows.append(pd.Series(f, index=edge_names))\n",
    "    if all_flows:\n",
    "        out = pd.concat(all_flows)\n",
    "        return out.reindex(out.abs().sort_values(ascending=False).index)\n",
    "    return pd.Series(dtype=float)\n",
    "\n",
    "# Optional: kurzer Check - 1 MW-Impuls sollte Flüsse in \"vernünftiger\" Größenordnung geben\n",
    "print(\"Sanity (1 MW in JUBO_E01):\")\n",
    "print(flows_from_impulse(\"JUBO_E01\", 1.0).head(5).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bfacc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max sicherer Transfer JUBO_E01 -> SHUW_E24:  α=5%: 8.94 MW  |  α=1%: 6.30 MW\n",
      "Bindende Kanten (5%): ['e:SHUW_E24--JUBO_A5']\n",
      "Bindende Kanten (1%): ['e:SHUW_E24--JUBO_A5']\n"
     ]
    }
   ],
   "source": [
    "# === TPTDF (A->B) und Paar-Bänder =============================================\n",
    "\n",
    "def locate_component_for_nodes(A_name: str, B_name: str):\n",
    "    \"\"\"Finde die PTDF-Komponente, in der A und B gemeinsam liegen.\"\"\"\n",
    "    for comp in ptdf_components:\n",
    "        nodes = comp[\"nodes\"]\n",
    "        if (A_name in nodes) and (B_name in nodes):\n",
    "            return comp\n",
    "    raise ValueError(f\"A={A_name} und B={B_name} liegen nicht in derselben Netzkomponente.\")\n",
    "\n",
    "def tptdf_vector(A_name: str, B_name: str, comp) -> pd.Series:\n",
    "    \"\"\"1 MW Transfer A->B (Slack-frei, slack-invariant): liefert Fluss je Leitung in der Komponente.\"\"\"\n",
    "    nodes = comp[\"nodes\"]; keep = comp[\"keep\"]; Hm = comp[\"PTDF\"]; edges = comp[\"edges\"]\n",
    "    a = nodes.index(A_name); b = nodes.index(B_name)\n",
    "    pr = np.zeros(len(keep))\n",
    "    for i, k in enumerate(keep):\n",
    "        if k == a: pr[i] = +1.0\n",
    "        elif k == b: pr[i] = -1.0\n",
    "    f = Hm @ pr\n",
    "    return pd.Series(f, index=[f\"e:{u}--{v}\" for (u,v) in edges])\n",
    "\n",
    "def pair_transfer_limit(A_name: str, B_name: str, alpha: float = 0.05) -> tuple[float, list]:\n",
    "    \"\"\"\n",
    "    Maximaler sicherer Transfer (MW) von A nach B unter Chance-Constraint (alpha).\n",
    "    Nutzt die robusten Limits + z*σ_e-Abschlag aus deiner stochastischen Band-Logik.\n",
    "    Rückgabe: (MW_limit, bindende_kanten)\n",
    "    \"\"\"\n",
    "    comp = locate_component_for_nodes(A_name, B_name)\n",
    "    f_unit = tptdf_vector(A_name, B_name, comp)  # Fluss je 1 MW Transfer\n",
    "\n",
    "    # Nur aktive Kanten betrachten (vereinheitlicht mit deiner Bandlogik)\n",
    "    f_unit = f_unit.reindex(active_edges).fillna(0.0)\n",
    "\n",
    "    # Effektive Grenzen: Fcap_eff = Fmax_robust - z * σ_e - |f0|\n",
    "    z = {0.05: 1.645, 0.01: 2.326}.get(alpha, 1.645)\n",
    "    Fsig_act = F_hist.std(axis=0).reindex(active_edges).fillna(0.0)\n",
    "    Fcap_eff = (Fmax_robust - z * Fsig_act - f0.abs()).reindex(active_edges).fillna(0.0)\n",
    "    Fcap_eff = Fcap_eff.clip(lower=0.0)\n",
    "\n",
    "    ratios = []\n",
    "    for e in active_edges:\n",
    "        denom = abs(f_unit.get(e, 0.0))\n",
    "        if denom <= 1e-12:\n",
    "            continue\n",
    "        ratios.append((Fcap_eff[e] / denom, e))\n",
    "    if not ratios:\n",
    "        return 0.0, []\n",
    "\n",
    "    limit = float(min(ratios, key=lambda t: t[0])[0])\n",
    "    binders = [e for v,e in ratios if v <= 1.01*limit]  # binnen 1% am Limit\n",
    "    return limit, binders\n",
    "\n",
    "# Beispiel: zulässiger Transfer JUBO_E01 -> SHUW_E24\n",
    "lim_5, bind_5 = pair_transfer_limit(\"JUBO_E01\", \"SHUW_E24\", alpha=0.05)\n",
    "lim_1, bind_1 = pair_transfer_limit(\"JUBO_E01\", \"SHUW_E24\", alpha=0.01)\n",
    "print(f\"\\nMax sicherer Transfer JUBO_E01 -> SHUW_E24:  α=5%: {lim_5:.2f} MW  |  α=1%: {lim_1:.2f} MW\")\n",
    "print(\"Bindende Kanten (5%):\", bind_5)\n",
    "print(\"Bindende Kanten (1%):\", bind_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f02ec",
   "metadata": {},
   "source": [
    "# Neue Idee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b1408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messknoten (uw_field/battery): ['BOLS_E42', 'JUBO_E01', 'JUBO_E02', 'JUBO_E03', 'SHUW_E24', 'Tarp_E01']\n",
      "Gemeinsame Zeitpunkte: 1343 | Messknoten im Datensatz: 6\n",
      "PTDF für 1 Komponente(n) aufgebaut.\n",
      "\n",
      "=== Auslastung zum letzten Zeitpunkt (2025-10-21 05:00:00) ===\n",
      "                                 element_id      type from_node   to_node  \\\n",
      "3   110_SHUW_TARP_GELB_JUBO_JUBO_A5_BOLS_A5      edge   BOLS_A5   JUBO_A5   \n",
      "7      110_SHUW_TARP_GELB_JUBO_BOLS_A5_TARP      edge  Tarp_E01   BOLS_A5   \n",
      "18                                 Tarp_E01  uw_field      None      None   \n",
      "11                                 SHUW_E24  uw_field      None      None   \n",
      "6      110_SHUW_TARP_GELB_JUBO_BOLS_A5_BOLS      edge   BOLS_A5  BOLS_E42   \n",
      "16                                 BOLS_E42   battery      None      None   \n",
      "1      110_SHUW_TARP_GELB_JUBO_SHUW_JUBO_A5      edge  SHUW_E24   JUBO_A5   \n",
      "2      110_SHUW_TARP_GELB_JUBO_JUBO_A5_JUBO      edge   JUBO_A5  JUBO_E01   \n",
      "0                               SHUW_SS_E24      edge      SHUW  SHUW_E24   \n",
      "4                             JUBO_E01_JUBO      edge  JUBO_E01      JUBO   \n",
      "5                             JUBO_E03_JUBO      edge      JUBO  JUBO_E03   \n",
      "8                          TARP_SS_TARP_E01      edge      Tarp  Tarp_E01   \n",
      "9                             JUBO_E02_JUBO      edge  JUBO_E02      JUBO   \n",
      "10                                     SHUW    busbar      None      None   \n",
      "12                                  JUBO_A5  junction      None      None   \n",
      "13                                 JUBO_E01  uw_field      None      None   \n",
      "14                                 JUBO_E03  uw_field      None      None   \n",
      "15                                  BOLS_A5  junction      None      None   \n",
      "17                                     Tarp    busbar      None      None   \n",
      "19                                 JUBO_E02  uw_field      None      None   \n",
      "20                                     JUBO    busbar      None      None   \n",
      "\n",
      "             timestamp    P_MW   U_kV      I_A  I_limit_A  utilization  \n",
      "3  2025-10-21 05:00:00 -17.467  110.0   96.503      555.0        0.174  \n",
      "7  2025-10-21 05:00:00 -27.200  110.0  150.277      985.0        0.153  \n",
      "18 2025-10-21 05:00:00  27.200  110.0  150.277     1100.0        0.137  \n",
      "11 2025-10-21 05:00:00 -40.572  110.0  224.157     2000.0        0.112  \n",
      "6  2025-10-21 05:00:00   9.733  110.0   53.773      547.0        0.098  \n",
      "16 2025-10-21 05:00:00  -9.733  110.0   53.773     1200.0        0.045  \n",
      "1  2025-10-21 05:00:00 -17.467  110.0   96.503     2212.0        0.044  \n",
      "2  2025-10-21 05:00:00  -0.000  110.0    0.000      632.0        0.000  \n",
      "0  2025-10-21 05:00:00  23.105  110.0  127.654        NaN          NaN  \n",
      "4  2025-10-21 05:00:00  23.695  110.0  130.913        NaN          NaN  \n",
      "5  2025-10-21 05:00:00  -5.174  110.0   28.586        NaN          NaN  \n",
      "8  2025-10-21 05:00:00   0.000  110.0    0.000        NaN          NaN  \n",
      "9  2025-10-21 05:00:00 -18.521  110.0  102.327        NaN          NaN  \n",
      "10 2025-10-21 05:00:00   0.000  110.0    0.000        NaN          NaN  \n",
      "12 2025-10-21 05:00:00   0.000  110.0    0.000        NaN          NaN  \n",
      "13 2025-10-21 05:00:00  23.695  110.0  130.913        NaN          NaN  \n",
      "14 2025-10-21 05:00:00  -5.174  110.0   28.586        NaN          NaN  \n",
      "15 2025-10-21 05:00:00   0.000  110.0    0.000        NaN          NaN  \n",
      "17 2025-10-21 05:00:00   0.000  110.0    0.000        NaN          NaN  \n",
      "19 2025-10-21 05:00:00 -18.521  110.0  102.327        NaN          NaN  \n",
      "20 2025-10-21 05:00:00   0.000  110.0    0.000        NaN          NaN  \n",
      "\n",
      "→ Exportiert: C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\graph\\auslastung_last_timestamp.csv\n",
      "\n",
      "=== Auslastung zum letzten Zeitpunkt (2025-10-21 05:00:00) ===\n",
      "                                 element_id      type           timestamp  \\\n",
      "3   110_SHUW_TARP_GELB_JUBO_JUBO_A5_BOLS_A5      edge 2025-10-21 05:00:00   \n",
      "7      110_SHUW_TARP_GELB_JUBO_BOLS_A5_TARP      edge 2025-10-21 05:00:00   \n",
      "18                                 Tarp_E01  uw_field 2025-10-21 05:00:00   \n",
      "11                                 SHUW_E24  uw_field 2025-10-21 05:00:00   \n",
      "6      110_SHUW_TARP_GELB_JUBO_BOLS_A5_BOLS      edge 2025-10-21 05:00:00   \n",
      "16                                 BOLS_E42   battery 2025-10-21 05:00:00   \n",
      "1      110_SHUW_TARP_GELB_JUBO_SHUW_JUBO_A5      edge 2025-10-21 05:00:00   \n",
      "2      110_SHUW_TARP_GELB_JUBO_JUBO_A5_JUBO      edge 2025-10-21 05:00:00   \n",
      "0                               SHUW_SS_E24      edge 2025-10-21 05:00:00   \n",
      "4                             JUBO_E01_JUBO      edge 2025-10-21 05:00:00   \n",
      "5                             JUBO_E03_JUBO      edge 2025-10-21 05:00:00   \n",
      "8                          TARP_SS_TARP_E01      edge 2025-10-21 05:00:00   \n",
      "9                             JUBO_E02_JUBO      edge 2025-10-21 05:00:00   \n",
      "10                                     SHUW    busbar 2025-10-21 05:00:00   \n",
      "12                                  JUBO_A5  junction 2025-10-21 05:00:00   \n",
      "13                                 JUBO_E01  uw_field 2025-10-21 05:00:00   \n",
      "14                                 JUBO_E03  uw_field 2025-10-21 05:00:00   \n",
      "15                                  BOLS_A5  junction 2025-10-21 05:00:00   \n",
      "17                                     Tarp    busbar 2025-10-21 05:00:00   \n",
      "19                                 JUBO_E02  uw_field 2025-10-21 05:00:00   \n",
      "20                                     JUBO    busbar 2025-10-21 05:00:00   \n",
      "\n",
      "      P_MW   U_kV      I_A  I_limit_A  utilization from_node   to_node  \\\n",
      "3  -17.467  110.0   96.503      555.0        0.174   BOLS_A5   JUBO_A5   \n",
      "7  -27.200  110.0  150.277      985.0        0.153  Tarp_E01   BOLS_A5   \n",
      "18  27.200  110.0  150.277     1100.0        0.137      None      None   \n",
      "11 -40.572  110.0  224.157     2000.0        0.112      None      None   \n",
      "6    9.733  110.0   53.773      547.0        0.098   BOLS_A5  BOLS_E42   \n",
      "16  -9.733  110.0   53.773     1200.0        0.045      None      None   \n",
      "1  -17.467  110.0   96.503     2212.0        0.044  SHUW_E24   JUBO_A5   \n",
      "2   -0.000  110.0    0.000      632.0        0.000   JUBO_A5  JUBO_E01   \n",
      "0   23.105  110.0  127.654        NaN          NaN      SHUW  SHUW_E24   \n",
      "4   23.695  110.0  130.913        NaN          NaN  JUBO_E01      JUBO   \n",
      "5   -5.174  110.0   28.586        NaN          NaN      JUBO  JUBO_E03   \n",
      "8    0.000  110.0    0.000        NaN          NaN      Tarp  Tarp_E01   \n",
      "9  -18.521  110.0  102.327        NaN          NaN  JUBO_E02      JUBO   \n",
      "10   0.000  110.0    0.000        NaN          NaN      None      None   \n",
      "12   0.000  110.0    0.000        NaN          NaN      None      None   \n",
      "13  23.695  110.0  130.913        NaN          NaN      None      None   \n",
      "14  -5.174  110.0   28.586        NaN          NaN      None      None   \n",
      "15   0.000  110.0    0.000        NaN          NaN      None      None   \n",
      "17   0.000  110.0    0.000        NaN          NaN      None      None   \n",
      "19 -18.521  110.0  102.327        NaN          NaN      None      None   \n",
      "20   0.000  110.0    0.000        NaN          NaN      None      None   \n",
      "\n",
      "                               limit_source  \n",
      "3   110_SHUW_TARP_GELB_JUBO_JUBO_A5_BOLS_A5  \n",
      "7      110_SHUW_TARP_GELB_JUBO_BOLS_A5_TARP  \n",
      "18                                 Tarp_E01  \n",
      "11                                 SHUW_E24  \n",
      "6      110_SHUW_TARP_GELB_JUBO_BOLS_A5_BOLS  \n",
      "16                                 BOLS_E42  \n",
      "1      110_SHUW_TARP_GELB_JUBO_SHUW_JUBO_A5  \n",
      "2      110_SHUW_TARP_GELB_JUBO_JUBO_A5_JUBO  \n",
      "0                                  SHUW_E24  \n",
      "4                                      None  \n",
      "5                                      None  \n",
      "8                                  Tarp_E01  \n",
      "9                                      None  \n",
      "10                                     None  \n",
      "12                                     None  \n",
      "13                                     None  \n",
      "14                                     None  \n",
      "15                                     None  \n",
      "17                                     None  \n",
      "19                                     None  \n",
      "20                                     None  \n",
      "\n",
      "→ Exportiert: C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\graph\\auslastung_last_timestamp.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------- Pfade & Konstanten ---------------------\n",
    "GRAPH_JSON = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\graph\\graph_with_jubo_e01.json\")\n",
    "MEAS_DIR   = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\handling_graph\\out\\nodes\")\n",
    "TS_COL     = \"timestamp\"\n",
    "VAL_COL    = \"P_Datapoint_ID\"\n",
    "MIN_SAMPLES = 200\n",
    "\n",
    "# Fixe Annahmen für Stromberechnung\n",
    "U_KV_DEFAULT = 110.0   # Leitungsebene\n",
    "COS_PHI      = 0.95\n",
    "\n",
    "# --------------------- Graph laden & Limits korrekt auslesen ---------------------\n",
    "elems = json.loads(GRAPH_JSON.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "G = nx.Graph()\n",
    "node_meta = {}\n",
    "edge_meta = {}\n",
    "\n",
    "for el in elems:\n",
    "    d = el.get(\"data\", {})\n",
    "    f = d.get(\"features\", {})  # <--- NEU: features-Block einbeziehen\n",
    "\n",
    "    # 1) Edges (mit source/target)\n",
    "    if \"source\" in d and \"target\" in d:\n",
    "        u, v = d[\"source\"], d[\"target\"]\n",
    "        edge_id = d.get(\"id\", f\"{u}__{v}\")\n",
    "        limit_A = f.get(\"Strom_Limit_in_A\") or d.get(\"Strom_Limit_in_A\")  # erst aus features, dann fallback\n",
    "        try:\n",
    "            limit_A = float(limit_A) if limit_A is not None else None\n",
    "        except Exception:\n",
    "            limit_A = None\n",
    "\n",
    "        G.add_edge(u, v, id=edge_id, x=1.0, Strom_Limit_in_A=limit_A)\n",
    "        edge_meta[edge_id] = {\"u\": u, \"v\": v, \"Strom_Limit_in_A\": limit_A}\n",
    "\n",
    "        # Sicherheitshalber Nodes anlegen\n",
    "        if u not in G.nodes:\n",
    "            G.add_node(u)\n",
    "        if v not in G.nodes:\n",
    "            G.add_node(v)\n",
    "\n",
    "    # 2) Nodes (uw_field, battery, etc.)\n",
    "    elif \"id\" in d:\n",
    "        nid = d[\"id\"]\n",
    "        ntype = d.get(\"type\")\n",
    "        limit_A = f.get(\"Strom_Limit_in_A\") or d.get(\"Strom_Limit_in_A\")\n",
    "        try:\n",
    "            limit_A = float(limit_A) if limit_A is not None else None\n",
    "        except Exception:\n",
    "            limit_A = None\n",
    "\n",
    "        G.add_node(nid, ntype=ntype, Strom_Limit_in_A=limit_A)\n",
    "        node_meta[nid] = {\"type\": ntype, \"Strom_Limit_in_A\": limit_A}\n",
    "\n",
    "# --------------------- Messknoten bestimmen (uw_field + battery) ---------------------\n",
    "meas_nodes = [nid for nid, meta in node_meta.items() if meta.get(\"type\") in {\"uw_field\", \"battery\"}]\n",
    "print(f\"Messknoten (uw_field/battery): {sorted(meas_nodes)}\")\n",
    "\n",
    "# --------------------- Zeitreihen laden (LEVELS in MW) ---------------------\n",
    "def load_series(name: str) -> pd.Series:\n",
    "    p = MEAS_DIR / f\"{name}.csv\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(p)\n",
    "    df[TS_COL] = pd.to_datetime(df[TS_COL], errors=\"coerce\", utc=True)\n",
    "    s = df.dropna(subset=[TS_COL, VAL_COL]).set_index(TS_COL)[VAL_COL].astype(float)\n",
    "    return s.tz_convert(None).sort_index()\n",
    "\n",
    "series = {}\n",
    "for n in meas_nodes:\n",
    "    s = load_series(n)\n",
    "    if s is not None:\n",
    "        series[n] = s\n",
    "\n",
    "if not series:\n",
    "    raise RuntimeError(\"Keine Messreihen gefunden.\")\n",
    "\n",
    "df_all = pd.concat(series.values(), axis=1, join=\"inner\")\n",
    "df_all.columns = list(series.keys())\n",
    "print(f\"Gemeinsame Zeitpunkte: {len(df_all)} | Messknoten im Datensatz: {df_all.shape[1]}\")\n",
    "if len(df_all) < MIN_SAMPLES:\n",
    "    print(\"Warnung: Wenige gemeinsame Zeitpunkte. Prüfe Datenabdeckung.\")\n",
    "\n",
    "# --------------------- PTDF je Komponente (x=1, Slack = Busbar falls vorhanden) ---------------------\n",
    "def build_ptdf_components(G: nx.Graph):\n",
    "    comps = []\n",
    "    for nodes in nx.connected_components(G):\n",
    "        Gc = G.subgraph(nodes).copy()\n",
    "        nodes = list(Gc.nodes())\n",
    "        edges = list(Gc.edges())\n",
    "        if not nodes or not edges:\n",
    "            continue\n",
    "        idx = {n:i for i,n in enumerate(nodes)}\n",
    "        m, n = len(edges), len(nodes)\n",
    "        # Inzidenz\n",
    "        A = np.zeros((m, n))\n",
    "        edge_ids = []\n",
    "        for e_i, (u, v) in enumerate(edges):\n",
    "            A[e_i, idx[u]] = +1.0\n",
    "            A[e_i, idx[v]] = -1.0\n",
    "            edge_ids.append(Gc[u][v].get(\"id\", f\"{u}__{v}\"))\n",
    "        # Slack\n",
    "        busbars = [n for n in nodes if Gc.nodes[n].get(\"ntype\") == \"busbar\"]\n",
    "        slack_node = busbars[0] if busbars else nodes[0]\n",
    "        slack_idx = idx[slack_node]\n",
    "        keep = [i for i in range(n) if i != slack_idx]\n",
    "        # PTDF\n",
    "        B = A.T @ A\n",
    "        B_rr = B[np.ix_(keep, keep)]\n",
    "        H = A[:, keep] @ np.linalg.pinv(B_rr)\n",
    "        comps.append({\n",
    "            \"graph\": Gc,\n",
    "            \"nodes\": nodes,\n",
    "            \"edges\": edges,\n",
    "            \"edge_ids\": edge_ids,        \n",
    "            \"A\": A,\n",
    "            \"PTDF\": H,\n",
    "            \"keep\": keep,\n",
    "            \"slack_idx\": slack_idx,\n",
    "            \"slack_node\": slack_node\n",
    "        })\n",
    "    return comps\n",
    "\n",
    "ptdf_components = build_ptdf_components(G)\n",
    "print(f\"PTDF für {len(ptdf_components)} Komponente(n) aufgebaut.\")\n",
    "if not ptdf_components:\n",
    "    raise RuntimeError(\"Keine PTDF-Komponente berechnet.\")\n",
    "\n",
    "# --------------------- Flüsse aus LEVELS P(t) (MW) ---------------------\n",
    "def compute_flows_from_levels(df_levels: pd.DataFrame) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for comp in ptdf_components:\n",
    "        nodes = comp[\"nodes\"]; keep = comp[\"keep\"]; Hm = comp[\"PTDF\"]; sidx = comp[\"slack_idx\"]\n",
    "        colmap = {c:i for i,c in enumerate(nodes)}\n",
    "        # baue P(t) für diese Komponente\n",
    "        X = df_levels.reindex(columns=nodes, fill_value=0.0).copy()\n",
    "        P = X.values  # (T x n)\n",
    "        # Slack-Bilanz je Zeitstempel\n",
    "        P[:, sidx] = - (P.sum(axis=1) - P[:, sidx])\n",
    "        P_r = P[:, keep]         # (T x (n-1))\n",
    "        F = (P_r @ Hm.T)         # (T x m)\n",
    "        cols = comp[\"edge_ids\"]  # echte Edge-IDs\n",
    "        frames.append(pd.DataFrame(F, index=X.index, columns=cols))\n",
    "    return pd.concat(frames, axis=1)\n",
    "\n",
    "F_hist_levels = compute_flows_from_levels(df_all)   # gesamte Historie auf Flüsse (MW)\n",
    "\n",
    "# --------------------- Strom & Auslastung am letzten Timestamp ---------------------\n",
    "def mw_to_ampere(P_MW: pd.Series, U_kV: float = U_KV_DEFAULT, cos_phi: float = COS_PHI) -> pd.Series:\n",
    "    # I[A] = P[MW]*1e6 / (sqrt(3)*U[V]*cosφ) = P[MW]*1e3 / (√3*U[kV]*cosφ)\n",
    "    denom = (np.sqrt(3.0) * U_kV * cos_phi)\n",
    "    return (P_MW * 1e3) / denom\n",
    "\n",
    "last_ts = df_all.index.max()\n",
    "\n",
    "# a) Edge-Report: Flüsse (MW) -> Ströme (A) -> Auslastung\n",
    "F_last = F_hist_levels.loc[last_ts]  # Serie je Edge-ID in MW\n",
    "I_edge = mw_to_ampere(F_last.abs())  # Betrag in A\n",
    "# Limits je Edge mappen\n",
    "edge_limits = pd.Series({eid: edge_meta.get(eid, {}).get(\"Strom_Limit_in_A\") for eid in F_last.index})\n",
    "edge_util = I_edge / edge_limits\n",
    "edge_rows = pd.DataFrame({\n",
    "    \"element_id\": F_last.index,\n",
    "    \"type\": \"edge\",\n",
    "    \"timestamp\": last_ts,\n",
    "    \"P_MW\": F_last.values,\n",
    "    \"U_kV\": U_KV_DEFAULT,\n",
    "    \"I_A\": I_edge.values,\n",
    "    \"I_limit_A\": edge_limits.values,\n",
    "    \"utilization\": edge_util.values\n",
    "})\n",
    "\n",
    "# b) Node-Report (uw_field + battery + optional andere Node-Typen mit Limits)\n",
    "#    P_MW = Einspeisung am Knoten (positiv/negativ je Messung), Strom daraus\n",
    "node_last = df_all.loc[last_ts].reindex(list(node_meta.keys())).fillna(0.0)\n",
    "node_types = pd.Series({nid: node_meta[nid].get(\"type\") for nid in node_meta})\n",
    "node_limits = pd.Series({nid: node_meta[nid].get(\"Strom_Limit_in_A\") for nid in node_meta})\n",
    "# nur Knoten mit Messwerten (meas_nodes), sonst P=0\n",
    "node_last = node_last.reindex(node_types.index).fillna(0.0)\n",
    "\n",
    "I_node = mw_to_ampere(node_last.abs())\n",
    "node_util = I_node / node_limits\n",
    "node_rows = pd.DataFrame({\n",
    "    \"element_id\": node_last.index,\n",
    "    \"type\": node_types.values,\n",
    "    \"timestamp\": last_ts,\n",
    "    \"P_MW\": node_last.values,\n",
    "    \"U_kV\": U_KV_DEFAULT,\n",
    "    \"I_A\": I_node.values,\n",
    "    \"I_limit_A\": node_limits.values,\n",
    "    \"utilization\": node_util.values\n",
    "})\n",
    "\n",
    "# c) Kombinierter Report (mit klarer Bezeichnung)\n",
    "edge_rows = edge_rows.copy()\n",
    "node_rows = node_rows.copy()\n",
    "\n",
    "# --- zusätzliche Spalten für Klarheit ---\n",
    "# für Edges: Start- und Zielknoten ergänzen (aus edge_meta)\n",
    "edge_rows[\"from_node\"] = edge_rows[\"element_id\"].map(lambda eid: edge_meta.get(eid, {}).get(\"u\"))\n",
    "edge_rows[\"to_node\"]   = edge_rows[\"element_id\"].map(lambda eid: edge_meta.get(eid, {}).get(\"v\"))\n",
    "\n",
    "# für Nodes: Typ direkt anzeigen (z. B. uw_field, battery)\n",
    "node_rows[\"from_node\"] = None\n",
    "node_rows[\"to_node\"]   = None\n",
    "\n",
    "# --- kombinieren ---\n",
    "report = pd.concat([edge_rows, node_rows], axis=0, ignore_index=True)\n",
    "\n",
    "# Numerik bereinigen\n",
    "def _to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "report[\"I_limit_A\"] = report[\"I_limit_A\"].map(_to_float)\n",
    "report[\"I_A\"] = report[\"I_A\"].astype(float)\n",
    "report[\"utilization\"] = report[\"I_A\"] / report[\"I_limit_A\"]\n",
    "# --- Herkunft des Limits (limit_source) ergänzen ---\n",
    "def find_limit_source(row):\n",
    "    if row[\"type\"] == \"edge\":\n",
    "        meta = edge_meta.get(row[\"element_id\"], {})\n",
    "        if meta.get(\"Strom_Limit_in_A\") is not None:\n",
    "            return row[\"element_id\"]\n",
    "        # Falls Edge kein Limit hat, aber einer der Endknoten schon:\n",
    "        u, v = meta.get(\"u\"), meta.get(\"v\")\n",
    "        if u in node_meta and node_meta[u].get(\"Strom_Limit_in_A\"):\n",
    "            return u\n",
    "        if v in node_meta and node_meta[v].get(\"Strom_Limit_in_A\"):\n",
    "            return v\n",
    "        return None\n",
    "    else:\n",
    "        # Für Nodes: eigene ID, falls Limit existiert\n",
    "        if node_meta.get(row[\"element_id\"], {}).get(\"Strom_Limit_in_A\"):\n",
    "            return row[\"element_id\"]\n",
    "        return None\n",
    "\n",
    "report[\"limit_source\"] = report.apply(find_limit_source, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# --- Sortieren ---\n",
    "report_sorted = report.sort_values([\"utilization\"], ascending=[False], na_position=\"last\")\n",
    "\n",
    "# --- Ausgabe ---\n",
    "cols_show = [\n",
    "    \"element_id\", \"type\", \"from_node\", \"to_node\",\n",
    "    \"timestamp\", \"P_MW\", \"U_kV\", \"I_A\", \"I_limit_A\", \"utilization\"\n",
    "]\n",
    "\n",
    "print(f\"\\n=== Auslastung zum letzten Zeitpunkt ({last_ts}) ===\")\n",
    "print(report_sorted[cols_show].head(25).round(3))\n",
    "\n",
    "# --- Export ---\n",
    "out_path = GRAPH_JSON.parent / \"auslastung_last_timestamp.csv\"\n",
    "report_sorted[cols_show].to_csv(out_path, index=False)\n",
    "print(f\"\\n→ Exportiert: {out_path}\")\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"\\n=== Auslastung zum letzten Zeitpunkt ({last_ts}) ===\")\n",
    "print(report_sorted.head(25).round(3))\n",
    "\n",
    "# Export\n",
    "out_path = GRAPH_JSON.parent / \"auslastung_last_timestamp.csv\"\n",
    "report_sorted.to_csv(out_path, index=False)\n",
    "print(f\"\\n→ Exportiert: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f0531",
   "metadata": {},
   "source": [
    "# Mit more printing of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11033f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1) KNOTEN UND LEITUNGEN ===\n",
      "Knoten: ['SHUW', 'SHUW_E24', 'JUBO_A5', 'JUBO_E01', 'JUBO_E03', 'BOLS_A5', 'BOLS_E42', 'Tarp', 'Tarp_E01', 'JUBO_E02', 'JUBO']\n",
      "Leitungen:\n",
      "  ('SHUW', 'SHUW_E24', {'x': 1.0, 'id': 'SHUW_SS_E24'})\n",
      "  ('SHUW_E24', 'JUBO_A5', {'x': 1.0, 'id': '110_SHUW_TARP_GELB_JUBO_SHUW_JUBO_A5'})\n",
      "  ('JUBO_A5', 'JUBO_E01', {'x': 1.0, 'id': '110_SHUW_TARP_GELB_JUBO_JUBO_A5_JUBO'})\n",
      "  ('JUBO_A5', 'BOLS_A5', {'x': 1.0, 'id': '110_SHUW_TARP_GELB_JUBO_JUBO_A5_BOLS_A5'})\n",
      "  ('JUBO_E01', 'JUBO', {'x': 1.0, 'id': 'JUBO_E01_JUBO'})\n",
      "  ('JUBO_E03', 'JUBO', {'x': 1.0, 'id': 'JUBO_E03_JUBO'})\n",
      "  ('BOLS_A5', 'BOLS_E42', {'x': 1.0, 'id': '110_SHUW_TARP_GELB_JUBO_BOLS_A5_BOLS'})\n",
      "  ('BOLS_A5', 'Tarp_E01', {'x': 1.0, 'id': '110_SHUW_TARP_GELB_JUBO_BOLS_A5_TARP'})\n",
      "  ('Tarp', 'Tarp_E01', {'x': 1.0, 'id': 'TARP_SS_TARP_E01'})\n",
      "  ('JUBO_E02', 'JUBO', {'x': 1.0, 'id': 'JUBO_E02_JUBO'})\n",
      "\n",
      "=== 2) INDEXE ===\n",
      "Knoten-Index: {'SHUW': 0, 'SHUW_E24': 1, 'JUBO_A5': 2, 'JUBO_E01': 3, 'JUBO_E03': 4, 'BOLS_A5': 5, 'BOLS_E42': 6, 'Tarp': 7, 'Tarp_E01': 8, 'JUBO_E02': 9, 'JUBO': 10}\n",
      "Anzahl Knoten = 11, Anzahl Leitungen = 10\n",
      "\n",
      "=== 3) INZIDENZMATRIX A (m x n) ===\n",
      "[[ 1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.  1. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1. -1.]]\n",
      "Reaktanzen je Leitung (x): [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Edge-IDs: ['SHUW_SS_E24', '110_SHUW_TARP_GELB_JUBO_SHUW_JUBO_A5', '110_SHUW_TARP_GELB_JUBO_JUBO_A5_JUBO', '110_SHUW_TARP_GELB_JUBO_JUBO_A5_BOLS_A5', 'JUBO_E01_JUBO', 'JUBO_E03_JUBO', '110_SHUW_TARP_GELB_JUBO_BOLS_A5_BOLS', '110_SHUW_TARP_GELB_JUBO_BOLS_A5_TARP', 'TARP_SS_TARP_E01', 'JUBO_E02_JUBO']\n",
      "\n",
      "=== 4) SLACK ===\n",
      "Slack-Knoten: SHUW (Index: 0 )\n",
      "\n",
      "=== 5) REDUZIERTE INZIDENZ A_r ===\n",
      "[[-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  1. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1. -1.]]\n",
      "Behaltene Knoten: ['SHUW_E24', 'JUBO_A5', 'JUBO_E01', 'JUBO_E03', 'BOLS_A5', 'BOLS_E42', 'Tarp', 'Tarp_E01', 'JUBO_E02', 'JUBO']\n",
      "\n",
      "=== 6) X^{-1} (Diagonalmatrix der Leitungsadmittanzen) ===\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "=== 7) REDUZIERTE BUS-MATRIX B_rr ===\n",
      "[[ 2. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [-1.  3. -1.  0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  2.  0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0. -1.  0.  0.  3. -1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0. -1.  2.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1. -1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0. -1.  3.]]\n",
      "\n",
      "=== 8) PSEUDOINVERSE VON B_rr ===\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [1. 2. 3. 3. 2. 2. 2. 2. 3. 3.]\n",
      " [1. 2. 3. 5. 2. 2. 2. 2. 4. 4.]\n",
      " [1. 2. 2. 2. 3. 3. 3. 3. 2. 2.]\n",
      " [1. 2. 2. 2. 3. 4. 3. 3. 2. 2.]\n",
      " [1. 2. 2. 2. 3. 3. 5. 4. 2. 2.]\n",
      " [1. 2. 2. 2. 3. 3. 4. 4. 2. 2.]\n",
      " [1. 2. 3. 4. 2. 2. 2. 2. 5. 4.]\n",
      " [1. 2. 3. 4. 2. 2. 2. 2. 4. 4.]]\n",
      "\n",
      "=== 9) PTDF-MATRIX ===\n",
      "Zeilen = Leitungen: ['SHUW_SS_E24', '110_SHUW_TARP_GELB_JUBO_SHUW_JUBO_A5', '110_SHUW_TARP_GELB_JUBO_JUBO_A5_JUBO', '110_SHUW_TARP_GELB_JUBO_JUBO_A5_BOLS_A5', 'JUBO_E01_JUBO', 'JUBO_E03_JUBO', '110_SHUW_TARP_GELB_JUBO_BOLS_A5_BOLS', '110_SHUW_TARP_GELB_JUBO_BOLS_A5_TARP', 'TARP_SS_TARP_E01', 'JUBO_E02_JUBO']\n",
      "Spalten = Knoten (ohne Slack): ['SHUW_E24', 'JUBO_A5', 'JUBO_E01', 'JUBO_E03', 'BOLS_A5', 'BOLS_E42', 'Tarp', 'Tarp_E01', 'JUBO_E02', 'JUBO']\n",
      "[[-1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  -1.00000000e+00 -1.00000000e+00]\n",
      " [-6.66133815e-16 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  -1.00000000e+00 -1.00000000e+00]\n",
      " [ 4.44089210e-16  1.33226763e-15 -1.00000000e+00 -1.00000000e+00\n",
      "   1.33226763e-15  1.77635684e-15  1.33226763e-15  1.55431223e-15\n",
      "  -1.00000000e+00 -1.00000000e+00]\n",
      " [ 0.00000000e+00  4.44089210e-16 -8.88178420e-16 -1.77635684e-15\n",
      "  -1.00000000e+00 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
      "  -8.88178420e-16 -8.88178420e-16]\n",
      " [-4.44089210e-16 -8.88178420e-16 -1.77635684e-15 -1.00000000e+00\n",
      "  -8.88178420e-16 -8.88178420e-16 -4.44089210e-16 -4.44089210e-16\n",
      "  -1.00000000e+00 -1.00000000e+00]\n",
      " [-2.22044605e-16  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "  -4.44089210e-16 -4.44089210e-16 -8.88178420e-16 -6.66133815e-16\n",
      "   8.88178420e-16 -8.88178420e-16]\n",
      " [-4.44089210e-16 -8.88178420e-16 -8.88178420e-16 -4.44089210e-16\n",
      "  -4.44089210e-16 -1.00000000e+00 -4.44089210e-16 -8.88178420e-16\n",
      "  -8.88178420e-16 -8.88178420e-16]\n",
      " [ 2.22044605e-16  4.44089210e-16  8.88178420e-16  8.88178420e-16\n",
      "   1.33226763e-15  1.77635684e-15 -1.00000000e+00 -1.00000000e+00\n",
      "   4.44089210e-16  4.44089210e-16]\n",
      " [-1.77635684e-15 -3.33066907e-15 -3.10862447e-15 -3.99680289e-15\n",
      "  -3.99680289e-15 -4.44089210e-15  1.00000000e+00 -4.44089210e-15\n",
      "  -3.99680289e-15 -3.99680289e-15]\n",
      " [ 0.00000000e+00  4.44089210e-16  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  4.44089210e-16  4.44089210e-16  2.22044605e-16\n",
      "   1.00000000e+00 -8.88178420e-16]]\n",
      "\n",
      "=== 10) TEST-INJEKTION ===\n",
      "+1 MW am Knoten 'SHUW_E24' (Slack 'SHUW' nimmt -1 MW auf)\n",
      "→ resultierende Flussänderungen je Leitung (in MW):\n",
      "  SHUW_SS_E24                          -1.00000\n",
      "  110_SHUW_TARP_GELB_JUBO_SHUW_JUBO_A5  -1.00000\n",
      "  110_SHUW_TARP_GELB_JUBO_JUBO_A5_JUBO  -1.00000\n",
      "  110_SHUW_TARP_GELB_JUBO_JUBO_A5_BOLS_A5  -0.00000\n",
      "  JUBO_E01_JUBO                        -0.00000\n",
      "  JUBO_E03_JUBO                         0.00000\n",
      "  110_SHUW_TARP_GELB_JUBO_BOLS_A5_BOLS  -0.00000\n",
      "  110_SHUW_TARP_GELB_JUBO_BOLS_A5_TARP   0.00000\n",
      "  TARP_SS_TARP_E01                     -0.00000\n",
      "  JUBO_E02_JUBO                         0.00000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 0) PFAD & EINSTELLUNGEN\n",
    "# ============================================================\n",
    "GRAPH_JSON = Path(r\"C:\\Users\\M97947\\OneDrive - E.ON\\Dokumente\\Thesis\\Code\\fca_leistungsbaender\\exploratory\\graph\\graph_with_jubo_e01.json\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) GRAPH LADEN UND AUFBAUEN\n",
    "# ============================================================\n",
    "elems = json.loads(GRAPH_JSON.read_text(encoding=\"utf-8\"))\n",
    "G = nx.Graph()\n",
    "\n",
    "for el in elems:\n",
    "    d = el.get(\"data\", {})\n",
    "    f = d.get(\"features\", {}) or {}\n",
    "\n",
    "    # --- Edges ---\n",
    "    if \"source\" in d and \"target\" in d:\n",
    "        u, v = d[\"source\"], d[\"target\"]\n",
    "        # Reaktanz (falls vorhanden), sonst 1.0\n",
    "        x_val = f.get(\"reactance\") or f.get(\"X\") or d.get(\"reactance\") or 1.0\n",
    "        try:\n",
    "            x_val = float(x_val)\n",
    "        except Exception:\n",
    "            x_val = 1.0\n",
    "\n",
    "        G.add_edge(u, v, x=x_val, id=d.get(\"id\", f\"{u}__{v}\"))\n",
    "\n",
    "        # Sicherheitshalber Knoten hinzufügen\n",
    "        if u not in G.nodes:\n",
    "            G.add_node(u, ntype=None)\n",
    "        if v not in G.nodes:\n",
    "            G.add_node(v, ntype=None)\n",
    "\n",
    "    # --- Nodes ---\n",
    "    elif \"id\" in d:\n",
    "        nid = d[\"id\"]\n",
    "        ntype = d.get(\"type\")\n",
    "        G.add_node(nid, ntype=ntype)\n",
    "\n",
    "print(\"=== 1) KNOTEN UND LEITUNGEN ===\")\n",
    "print(\"Knoten:\", list(G.nodes()))\n",
    "print(\"Leitungen:\")\n",
    "for e in G.edges(data=True):\n",
    "    print(\" \", e)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 2) INDEXE FESTLEGEN\n",
    "# ============================================================\n",
    "nodes = list(G.nodes())\n",
    "edges = list(G.edges())\n",
    "n = len(nodes)\n",
    "m = len(edges)\n",
    "idx = {nname: i for i, nname in enumerate(nodes)}\n",
    "\n",
    "print(\"=== 2) INDEXE ===\")\n",
    "print(\"Knoten-Index:\", idx)\n",
    "print(f\"Anzahl Knoten = {n}, Anzahl Leitungen = {m}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 3) INZIDENZMATRIX A AUFBAUEN\n",
    "# ============================================================\n",
    "A = np.zeros((m, n))\n",
    "x_list = []\n",
    "edge_ids = []\n",
    "\n",
    "for e_i, (u, v) in enumerate(edges):\n",
    "    A[e_i, idx[u]] = +1.0   # willkürliche Richtung u -> v\n",
    "    A[e_i, idx[v]] = -1.0\n",
    "    x_val = G[u][v].get(\"x\", 1.0)\n",
    "    try:\n",
    "        x_val = float(x_val)\n",
    "    except Exception:\n",
    "        x_val = 1.0\n",
    "    x_list.append(x_val)\n",
    "    edge_ids.append(G[u][v].get(\"id\", f\"{u}__{v}\"))\n",
    "\n",
    "print(\"=== 3) INZIDENZMATRIX A (m x n) ===\")\n",
    "print(A)\n",
    "print(\"Reaktanzen je Leitung (x):\", x_list)\n",
    "print(\"Edge-IDs:\", edge_ids)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 4) SLACK-KNOTEN BESTIMMEN\n",
    "# ============================================================\n",
    "busbars = [nn for nn in nodes if G.nodes[nn].get(\"ntype\") == \"busbar\"]\n",
    "if busbars:\n",
    "    slack_node = busbars[0]\n",
    "else:\n",
    "    slack_node = nodes[0]\n",
    "slack_idx = idx[slack_node]\n",
    "\n",
    "print(\"=== 4) SLACK ===\")\n",
    "print(\"Slack-Knoten:\", slack_node, \"(Index:\", slack_idx, \")\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 5) REDUZIERTE INZIDENZ A_r (SPALTE DES SLACK RAUS)\n",
    "# ============================================================\n",
    "keep = [i for i in range(n) if i != slack_idx]\n",
    "A_r = A[:, keep]\n",
    "\n",
    "print(\"=== 5) REDUZIERTE INZIDENZ A_r ===\")\n",
    "print(A_r)\n",
    "print(\"Behaltene Knoten:\", [nodes[i] for i in keep])\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 6) DIAGONALE X^{-1} (ADMINTTANZEN)\n",
    "# ============================================================\n",
    "X_inv = np.diag([1.0 / xv for xv in x_list])\n",
    "\n",
    "print(\"=== 6) X^{-1} (Diagonalmatrix der Leitungsadmittanzen) ===\")\n",
    "print(X_inv)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 7) REDUZIERTE BUS-MATRIX B_rr\n",
    "# ============================================================\n",
    "B_rr = A_r.T @ X_inv @ A_r\n",
    "\n",
    "print(\"=== 7) REDUZIERTE BUS-MATRIX B_rr ===\")\n",
    "print(B_rr)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 8) PSEUDOINVERSE B_rr_inv\n",
    "# ============================================================\n",
    "B_rr_inv = np.linalg.pinv(B_rr)\n",
    "\n",
    "print(\"=== 8) PSEUDOINVERSE VON B_rr ===\")\n",
    "print(B_rr_inv)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 9) PTDF BERECHNEN\n",
    "# ============================================================\n",
    "PTDF = X_inv @ A_r @ B_rr_inv\n",
    "\n",
    "print(\"=== 9) PTDF-MATRIX ===\")\n",
    "print(\"Zeilen = Leitungen:\", edge_ids)\n",
    "print(\"Spalten = Knoten (ohne Slack):\", [nodes[i] for i in keep])\n",
    "print(PTDF)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 10) TEST-INJEKTION\n",
    "# ============================================================\n",
    "test_node_idx = keep[0]\n",
    "test_node_name = nodes[test_node_idx]\n",
    "\n",
    "deltaP_r = np.zeros(len(keep))\n",
    "deltaP_r[0] = 1.0  # +1 MW Einspeisung am ersten Nicht-Slack-Knoten\n",
    "\n",
    "deltaF = PTDF @ deltaP_r\n",
    "\n",
    "print(\"=== 10) TEST-INJEKTION ===\")\n",
    "print(f\"+1 MW am Knoten '{test_node_name}' (Slack '{slack_node}' nimmt -1 MW auf)\")\n",
    "print(\"→ resultierende Flussänderungen je Leitung (in MW):\")\n",
    "for e_name, f_val in zip(edge_ids, deltaF):\n",
    "    print(f\"  {e_name:35s}  {f_val: .5f}\")\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
