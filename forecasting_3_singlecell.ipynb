{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENT CELL (HIGH node only) — Day-Ahead (H=100)\n",
    "# Improvements included:\n",
    "# - Balanced config (runtime + robustness)\n",
    "# - Clear HIGH-node selection printout\n",
    "# - Day-ahead evaluation on HOLDOUT for: Persistence + Recursive (Ridge/RF/HGB) + DirectStacked (best model)\n",
    "# - Horizon-bucket MAE + Skill score vs Persistence\n",
    "# - Direct-Stacked CV split by ORIGIN (not by stacked row order)\n",
    "# - Robust logging for skipped folds / effective folds used\n",
    "# - Save artifacts (csv/json + MAE(h) curves) into reports/forecast_experiments/<timestamp>/\n",
    "# ============================================================\n",
    "\n",
    "import warnings, math, time, json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from joblib import dump \n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "from src.config import CLEAN_TS_DIR\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Balanced / thesis-tauglich settings\n",
    "# =========================\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "TARGET = \"P_MW\"\n",
    "\n",
    "RAW_FEATURES = [\"wind_speed_mps\", \"solar_radiation_Wm2\", \"temperature_C\"]\n",
    "TARGET_LAGS = [1, 4, 8, 12, 24, 96]\n",
    "\n",
    "FOURIER_K = 3\n",
    "PERIOD_STEPS = 96\n",
    "\n",
    "MIN_COVERAGE = 0.20\n",
    "\n",
    "H = 100                          # day-ahead horizon (~25h)\n",
    "\n",
    "FINAL_HOLDOUT_FRAC = 0.20\n",
    "\n",
    "OUTER_SPLITS = 3\n",
    "INNER_SPLITS = 3\n",
    "N_ITER = 10\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = 1\n",
    "\n",
    "ORIGIN_STRIDE = 8                # every 2 hours (15min steps)\n",
    "MAX_TEST_ORIGINS_PER_FOLD = 400  # cap rollouts during model-selection\n",
    "\n",
    "# Horizon buckets for reporting\n",
    "H_BUCKETS = [\n",
    "    (1, 4),\n",
    "    (5, 12),\n",
    "    (13, 24),\n",
    "    (25, 48),\n",
    "    (49, 100),\n",
    "]\n",
    "\n",
    "# Output folder\n",
    "RUN_TS = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR = Path(\"reports\") / \"forecast_experiments\" / RUN_TS\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save wining models\n",
    "LATEST_DIR = Path(\"reports\") / \"forecast_experiments\" / \"_current\"\n",
    "LATEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def evaluate(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(math.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def bucket_means(mae_h, buckets):\n",
    "    out = {}\n",
    "    for a, b in buckets:\n",
    "        # horizons are 1-indexed in naming, but array is 0-indexed\n",
    "        seg = mae_h[(a-1):b]\n",
    "        out[f\"MAE_h{a:03d}-{b:03d}\"] = float(np.nanmean(seg))\n",
    "    return out\n",
    "\n",
    "def skill_score(mae_model, mae_baseline):\n",
    "    return float(1.0 - (mae_model / mae_baseline))\n",
    "\n",
    "def add_time_features(df):\n",
    "    ts = pd.to_datetime(df[TIMESTAMP_COL], utc=False)\n",
    "    df[\"hour\"] = ts.dt.hour\n",
    "    df[\"dayofweek\"] = ts.dt.dayofweek\n",
    "    df[\"month\"] = ts.dt.month\n",
    "\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"dow_sin\"]  = np.sin(2 * np.pi * df[\"dayofweek\"] / 7)\n",
    "    df[\"dow_cos\"]  = np.cos(2 * np.pi * df[\"dayofweek\"] / 7)\n",
    "    df[\"month_sin\"]= np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "    df[\"month_cos\"]= np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "    return df\n",
    "\n",
    "def add_fourier_features(df, period_steps=PERIOD_STEPS, K=FOURIER_K, prefix=\"day\"):\n",
    "    step_idx = (np.arange(len(df), dtype=float) % period_steps)\n",
    "    for k in range(1, K + 1):\n",
    "        df[f\"{prefix}_sin{k}\"] = np.sin(2 * np.pi * k * step_idx / period_steps)\n",
    "        df[f\"{prefix}_cos{k}\"] = np.cos(2 * np.pi * k * step_idx / period_steps)\n",
    "    return df\n",
    "\n",
    "def add_target_lags(df, target_col, lags):\n",
    "    for L in lags:\n",
    "        df[f\"{target_col}_lag{L}\"] = df[target_col].shift(L)\n",
    "    return df\n",
    "\n",
    "def prepare_df(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL], utc=False)\n",
    "    df = df.sort_values(TIMESTAMP_COL).reset_index(drop=True)\n",
    "\n",
    "    df = add_time_features(df)\n",
    "    df = add_fourier_features(df)\n",
    "    df = add_target_lags(df, TARGET, TARGET_LAGS)\n",
    "\n",
    "    df[\"target_1\"] = df[TARGET].shift(-1)\n",
    "\n",
    "    present_feats = [c for c in RAW_FEATURES if c in df.columns]\n",
    "    time_feats = [\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\"]\n",
    "    fourier_feats = [f\"day_sin{k}\" for k in range(1, FOURIER_K+1)] + [f\"day_cos{k}\" for k in range(1, FOURIER_K+1)]\n",
    "    lag_feats = [f\"{TARGET}_lag{L}\" for L in TARGET_LAGS]\n",
    "\n",
    "    exog_cols = present_feats + time_feats + fourier_feats\n",
    "    lag_cols = lag_feats\n",
    "    feature_cols = exog_cols + lag_cols\n",
    "\n",
    "    df_model = df.dropna(subset=feature_cols + [\"target_1\", TARGET]).copy().reset_index(drop=True)\n",
    "    return df_model, feature_cols, exog_cols, lag_cols, present_feats\n",
    "\n",
    "def make_models_and_spaces():\n",
    "    models = {}\n",
    "\n",
    "    models[\"Ridge\"] = (\n",
    "        Pipeline([(\"scaler\", StandardScaler()), (\"model\", Ridge(random_state=RANDOM_STATE))]),\n",
    "        {\"model__alpha\": np.logspace(-3, 3, 20).tolist()},\n",
    "    )\n",
    "\n",
    "    models[\"RF\"] = (\n",
    "        RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS),\n",
    "        {\n",
    "            \"n_estimators\": [200, 400, 800],\n",
    "            \"max_depth\": [None, 8, 12, 16, 24],\n",
    "            \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "            \"max_features\": [\"sqrt\", 0.5, 0.8, None],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    models[\"HGB\"] = (\n",
    "        HistGradientBoostingRegressor(random_state=RANDOM_STATE),\n",
    "        {\n",
    "            \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "            \"max_depth\": [None, 6, 10, 14],\n",
    "            \"max_leaf_nodes\": [31, 63, 127],\n",
    "            \"min_samples_leaf\": [10, 20, 50],\n",
    "            \"l2_regularization\": [0.0, 0.1, 1.0],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return models\n",
    "\n",
    "def _update_lags_from_history(lag_cols, history_values):\n",
    "    # lag col format: \"P_MW_lag96\" etc.\n",
    "    return np.array([history_values[int(col.split(\"lag\")[1])] for col in lag_cols], dtype=float)\n",
    "\n",
    "def evaluate_day_ahead_recursive(model, df_model, exog_cols, lag_cols, H=100, origin_indices=None, origin_stride=1):\n",
    "    n = len(df_model)\n",
    "    max_origin = n - H - 1\n",
    "    if max_origin <= 0:\n",
    "        raise ValueError(f\"Not enough data for H={H}. n={n}\")\n",
    "\n",
    "    if origin_indices is None:\n",
    "        origin_indices = np.arange(0, max_origin + 1, origin_stride, dtype=int)\n",
    "    else:\n",
    "        origin_indices = np.asarray(origin_indices, dtype=int)\n",
    "        origin_indices = origin_indices[origin_indices <= max_origin]\n",
    "\n",
    "    abs_errors_h = [[] for _ in range(H)]\n",
    "    sq_errors_h = [[] for _ in range(H)]\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    lag_steps = [int(c.split(\"lag\")[1]) for c in lag_cols]\n",
    "    max_lag = max(lag_steps)\n",
    "\n",
    "    for j in origin_indices:\n",
    "        # history[L] stores y(t-L+1) with L=1 being current y(t)\n",
    "        history = {}\n",
    "        for L in range(1, max_lag + 1):\n",
    "            idx = j - (L - 1)\n",
    "            history[L] = float(df_model[TARGET].iloc[idx]) if idx >= 0 else np.nan\n",
    "        if any(np.isnan(history[L]) for L in lag_steps):\n",
    "            continue\n",
    "\n",
    "        for k in range(1, H + 1):\n",
    "            step_idx = j + (k - 1)  # feature time\n",
    "            exog = df_model.loc[step_idx, exog_cols].astype(float).values if exog_cols else np.array([], dtype=float)\n",
    "            lag_vec = _update_lags_from_history(lag_cols, history)\n",
    "            x = np.concatenate([exog, lag_vec]).reshape(1, -1)\n",
    "\n",
    "            y_hat = float(model.predict(x)[0])\n",
    "            y_true = float(df_model[TARGET].iloc[j + k])\n",
    "\n",
    "            err = y_true - y_hat\n",
    "            abs_errors_h[k-1].append(abs(err))\n",
    "            sq_errors_h[k-1].append(err * err)\n",
    "\n",
    "            y_true_all.append(y_true)\n",
    "            y_pred_all.append(y_hat)\n",
    "\n",
    "            # shift lags and add prediction\n",
    "            for L in range(max_lag, 1, -1):\n",
    "                history[L] = history[L-1]\n",
    "            history[1] = y_hat\n",
    "\n",
    "    mae_h = np.array([np.mean(v) if len(v) else np.nan for v in abs_errors_h], dtype=float)\n",
    "    rmse_h = np.array([math.sqrt(np.mean(v)) if len(v) else np.nan for v in sq_errors_h], dtype=float)\n",
    "    overall = evaluate(np.array(y_true_all), np.array(y_pred_all))\n",
    "    return overall, mae_h, rmse_h\n",
    "\n",
    "def evaluate_day_ahead_persistence(df_model, H=100, origin_stride=1):\n",
    "    n = len(df_model)\n",
    "    max_origin = n - H - 1\n",
    "    origin_indices = np.arange(0, max_origin + 1, origin_stride, dtype=int)\n",
    "\n",
    "    abs_errors_h = [[] for _ in range(H)]\n",
    "    sq_errors_h = [[] for _ in range(H)]\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for j in origin_indices:\n",
    "        y0 = float(df_model[TARGET].iloc[j])\n",
    "        for k in range(1, H + 1):\n",
    "            y_true = float(df_model[TARGET].iloc[j + k])\n",
    "            y_hat = y0\n",
    "            err = y_true - y_hat\n",
    "            abs_errors_h[k-1].append(abs(err))\n",
    "            sq_errors_h[k-1].append(err * err)\n",
    "            y_true_all.append(y_true)\n",
    "            y_pred_all.append(y_hat)\n",
    "\n",
    "    mae_h = np.array([np.mean(v) if len(v) else np.nan for v in abs_errors_h], dtype=float)\n",
    "    rmse_h = np.array([math.sqrt(np.mean(v)) if len(v) else np.nan for v in sq_errors_h], dtype=float)\n",
    "    overall = evaluate(np.array(y_true_all), np.array(y_pred_all))\n",
    "    return overall, mae_h, rmse_h\n",
    "\n",
    "# ---------- Direct (stacked) dataset + CV by ORIGIN ----------\n",
    "def build_stacked_direct_dataset(df_model, exog_cols, lag_cols, H=100, origin_stride=8):\n",
    "    n = len(df_model)\n",
    "    max_origin = n - H - 1\n",
    "    origins = np.arange(0, max_origin + 1, origin_stride, dtype=int)\n",
    "\n",
    "    X_rows, y_rows, origin_ids = [], [], []\n",
    "\n",
    "    for j in origins:\n",
    "        lag_vec = df_model.loc[j, lag_cols].astype(float).values\n",
    "        if np.any(np.isnan(lag_vec)):\n",
    "            continue\n",
    "\n",
    "        for h in range(1, H + 1):\n",
    "            tgt_idx = j + h\n",
    "            exog_vec = df_model.loc[tgt_idx, exog_cols].astype(float).values if exog_cols else np.array([], dtype=float)\n",
    "            h_feat = np.array([h, h / H], dtype=float)\n",
    "            X_rows.append(np.concatenate([exog_vec, lag_vec, h_feat]))\n",
    "            y_rows.append(float(df_model[TARGET].iloc[tgt_idx]))\n",
    "            origin_ids.append(j)\n",
    "\n",
    "    Xs = np.asarray(X_rows, dtype=float)\n",
    "    ys = np.asarray(y_rows, dtype=float)\n",
    "    origin_ids = np.asarray(origin_ids, dtype=int)\n",
    "    return Xs, ys, origin_ids\n",
    "\n",
    "def make_origin_time_cv_splits(origin_ids, n_splits=3):\n",
    "    \"\"\"\n",
    "    Build sklearn-compatible CV splits (train_idx, test_idx) where splits happen on ORIGIN level.\n",
    "    This avoids leakage in stacked (t,h) rows by ensuring validation origins are strictly later.\n",
    "    \"\"\"\n",
    "    unique_origins = np.unique(origin_ids)\n",
    "    unique_origins.sort()\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = []\n",
    "    for tr_o_idx, te_o_idx in tss.split(unique_origins):\n",
    "        tr_origins = set(unique_origins[tr_o_idx])\n",
    "        te_origins = set(unique_origins[te_o_idx])\n",
    "\n",
    "        tr_idx = np.where(np.isin(origin_ids, list(tr_origins)))[0]\n",
    "        te_idx = np.where(np.isin(origin_ids, list(te_origins)))[0]\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def evaluate_day_ahead_direct(model, df_model, exog_cols, lag_cols, H=100, origin_stride=8):\n",
    "    n = len(df_model)\n",
    "    max_origin = n - H - 1\n",
    "    origin_indices = np.arange(0, max_origin + 1, origin_stride, dtype=int)\n",
    "\n",
    "    abs_errors_h = [[] for _ in range(H)]\n",
    "    sq_errors_h = [[] for _ in range(H)]\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for j in origin_indices:\n",
    "        lag_vec = df_model.loc[j, lag_cols].astype(float).values\n",
    "        if np.any(np.isnan(lag_vec)):\n",
    "            continue\n",
    "\n",
    "        X_batch, y_true_batch = [], []\n",
    "        for h in range(1, H + 1):\n",
    "            tgt_idx = j + h\n",
    "            exog_vec = df_model.loc[tgt_idx, exog_cols].astype(float).values if exog_cols else np.array([], dtype=float)\n",
    "            h_feat = np.array([h, h / H], dtype=float)\n",
    "            X_batch.append(np.concatenate([exog_vec, lag_vec, h_feat]))\n",
    "            y_true_batch.append(float(df_model[TARGET].iloc[tgt_idx]))\n",
    "\n",
    "        X_batch = np.asarray(X_batch, dtype=float)\n",
    "        y_true_batch = np.asarray(y_true_batch, dtype=float)\n",
    "\n",
    "        y_hat_batch = model.predict(X_batch).astype(float)\n",
    "\n",
    "        for h in range(1, H + 1):\n",
    "            err = y_true_batch[h-1] - y_hat_batch[h-1]\n",
    "            abs_errors_h[h-1].append(abs(err))\n",
    "            sq_errors_h[h-1].append(err * err)\n",
    "            y_true_all.append(y_true_batch[h-1])\n",
    "            y_pred_all.append(y_hat_batch[h-1])\n",
    "\n",
    "    mae_h = np.array([np.mean(v) if len(v) else np.nan for v in abs_errors_h], dtype=float)\n",
    "    rmse_h = np.array([math.sqrt(np.mean(v)) if len(v) else np.nan for v in sq_errors_h], dtype=float)\n",
    "    overall = evaluate(np.array(y_true_all), np.array(y_pred_all))\n",
    "    return overall, mae_h, rmse_h\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Select ONLY the single HIGH node (max abs(median(P_MW)))\n",
    "# =========================\n",
    "meas_dir = Path(CLEAN_TS_DIR)\n",
    "files = sorted(meas_dir.glob(\"*_hist_clean.csv\"))\n",
    "assert files, f\"Keine *_hist_clean.csv in {meas_dir}\"\n",
    "\n",
    "rows = []\n",
    "for p in files:\n",
    "    node_id = p.stem.replace(\"_hist_clean\", \"\")\n",
    "    df = pd.read_csv(p, parse_dates=[TIMESTAMP_COL]).sort_values(TIMESTAMP_COL)\n",
    "    if TARGET not in df.columns:\n",
    "        continue\n",
    "    s = df[TARGET].astype(float)\n",
    "\n",
    "    coverage = 1.0 - s.isna().mean()\n",
    "    if coverage < MIN_COVERAGE:\n",
    "        continue\n",
    "\n",
    "    med = float(s.median(skipna=True))\n",
    "    rows.append({\n",
    "        \"node_id\": node_id,\n",
    "        \"coverage\": float(coverage),\n",
    "        \"mean\": float(s.mean(skipna=True)),\n",
    "        \"median\": med,\n",
    "        \"abs_median\": abs(med),\n",
    "        \"p95\": float(s.quantile(0.95)),\n",
    "        \"std\": float(s.std(skipna=True)),\n",
    "        \"n_raw\": int(len(s)),\n",
    "    })\n",
    "\n",
    "stats = pd.DataFrame(rows)\n",
    "assert not stats.empty, \"Nach Coverage-Filter keine Nodes übrig. MIN_COVERAGE senken.\"\n",
    "\n",
    "stats = stats.sort_values(\"abs_median\").reset_index(drop=True)\n",
    "high_node = stats.iloc[-1][\"node_id\"]\n",
    "\n",
    "print(\"Auswahlkriterium: MAX abs(median(P_MW)) mit MIN_COVERAGE\", MIN_COVERAGE)\n",
    "print(\"HIGH node:\", high_node)\n",
    "print(\"HIGH node stats:\")\n",
    "display(stats[stats[\"node_id\"] == high_node])\n",
    "\n",
    "stats.to_csv(OUT_DIR / \"node_stats.csv\", index=False)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Load and prepare data for HIGH node\n",
    "# =========================\n",
    "path = meas_dir / f\"{high_node}_hist_clean.csv\"\n",
    "df_raw = pd.read_csv(path)\n",
    "\n",
    "df_model, feature_cols, exog_cols, lag_cols, present_feats = prepare_df(df_raw)\n",
    "\n",
    "print(f\"\\nPrepared supervised base for {high_node}: n={len(df_model)}\")\n",
    "print(\"Exog present:\", present_feats)\n",
    "print(\"Feature cols:\", len(feature_cols), \"(exog/time/fourier:\", len(exog_cols), \", lags:\", len(lag_cols), \")\")\n",
    "assert len(df_model) > (H + 500), f\"Time series too short for H={H}. n={len(df_model)}\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Define train/holdout split\n",
    "# =========================\n",
    "n = len(df_model)\n",
    "split_idx = int((1.0 - FINAL_HOLDOUT_FRAC) * n)\n",
    "\n",
    "holdout_start = split_idx\n",
    "holdout_max_origin_global = n - H - 1\n",
    "if holdout_start > holdout_max_origin_global:\n",
    "    raise ValueError(\"Holdout window too small for day-ahead evaluation. Reduce FINAL_HOLDOUT_FRAC or H.\")\n",
    "\n",
    "df_train_full = df_model.iloc[:split_idx].reset_index(drop=True)\n",
    "df_hold = df_model.iloc[holdout_start:].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal split: train [0:{split_idx}) | holdout [{split_idx}:{n})\")\n",
    "print(f\"Holdout length: {len(df_hold)} | allowed max origin in holdout-only frame: {len(df_hold) - H - 1}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Baseline: Persistence on HOLDOUT (same slicing/stride as models)\n",
    "# =========================\n",
    "baseline_overall, baseline_mae_h, baseline_rmse_h = evaluate_day_ahead_persistence(\n",
    "    df_model=df_hold,\n",
    "    H=H,\n",
    "    origin_stride=ORIGIN_STRIDE\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Model selection on TRAIN (tune one-step; select by day-ahead recursive on outer folds)\n",
    "# =========================\n",
    "models = make_models_and_spaces()\n",
    "\n",
    "X_train_full = df_train_full[feature_cols].astype(float).reset_index(drop=True)\n",
    "y_train_full = df_train_full[\"target_1\"].astype(float).reset_index(drop=True)\n",
    "\n",
    "outer = TimeSeriesSplit(n_splits=OUTER_SPLITS)\n",
    "cv_rows = []\n",
    "fold_skips = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"MODEL SELECTION (TRAIN): tune one-step, evaluate day-ahead recursive on outer folds\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for model_name, (estimator, param_dist) in models.items():\n",
    "    fold_scores = []\n",
    "    fold_fit_secs = []\n",
    "    used_folds = 0\n",
    "\n",
    "    for fold_i, (tr_idx, te_idx) in enumerate(outer.split(X_train_full), start=1):\n",
    "        X_tr, y_tr = X_train_full.iloc[tr_idx], y_train_full.iloc[tr_idx]\n",
    "\n",
    "        inner = TimeSeriesSplit(n_splits=INNER_SPLITS)\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=N_ITER,\n",
    "            scoring=\"neg_mean_absolute_error\",\n",
    "            cv=inner,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=N_JOBS,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        search.fit(X_tr, y_tr)\n",
    "        fit_s = time.time() - t0\n",
    "\n",
    "        best = search.best_estimator_\n",
    "\n",
    "        # Fold dataframe: concatenate train+test portion (for recursion start), then restrict origins to test-only\n",
    "        df_fold = df_train_full.iloc[np.r_[tr_idx, te_idx]].reset_index(drop=True)\n",
    "        test_start = len(tr_idx)\n",
    "        test_end = len(tr_idx) + len(te_idx) - 1\n",
    "\n",
    "        max_origin = len(df_fold) - H - 1\n",
    "        origin_start = test_start\n",
    "        origin_end = min(max_origin, test_end - H)\n",
    "\n",
    "        if origin_end < origin_start:\n",
    "            fold_skips.append({\"model\": model_name, \"outer_fold\": fold_i, \"reason\": \"test_window_too_short_for_H\"})\n",
    "            continue\n",
    "\n",
    "        origins = np.arange(origin_start, origin_end + 1, ORIGIN_STRIDE, dtype=int)\n",
    "        if MAX_TEST_ORIGINS_PER_FOLD is not None and len(origins) > MAX_TEST_ORIGINS_PER_FOLD:\n",
    "            origins = origins[:MAX_TEST_ORIGINS_PER_FOLD]\n",
    "\n",
    "        overall, mae_h, rmse_h = evaluate_day_ahead_recursive(\n",
    "            model=best,\n",
    "            df_model=df_fold,\n",
    "            exog_cols=exog_cols,\n",
    "            lag_cols=lag_cols,\n",
    "            H=H,\n",
    "            origin_indices=origins,\n",
    "            origin_stride=ORIGIN_STRIDE\n",
    "        )\n",
    "\n",
    "        used_folds += 1\n",
    "        fold_scores.append(overall)\n",
    "        fold_fit_secs.append(fit_s)\n",
    "\n",
    "        cv_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"outer_fold\": fold_i,\n",
    "            \"dayahead_MAE\": overall[\"MAE\"],\n",
    "            \"dayahead_RMSE\": overall[\"RMSE\"],\n",
    "            \"dayahead_R2\": overall[\"R2\"],\n",
    "            \"fit_seconds\": float(fit_s),\n",
    "            \"best_params\": str(search.best_params_),\n",
    "            \"n_test_origins\": int(len(origins)),\n",
    "        })\n",
    "\n",
    "    # Mean summary row (only over used folds)\n",
    "    if used_folds > 0:\n",
    "        cv_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"outer_fold\": \"MEAN\",\n",
    "            \"dayahead_MAE\": float(np.mean([s[\"MAE\"] for s in fold_scores])),\n",
    "            \"dayahead_RMSE\": float(np.mean([s[\"RMSE\"] for s in fold_scores])),\n",
    "            \"dayahead_R2\": float(np.mean([s[\"R2\"] for s in fold_scores])),\n",
    "            \"fit_seconds\": float(np.mean(fold_fit_secs)) if fold_fit_secs else np.nan,\n",
    "            \"best_params\": \"(varies per fold)\",\n",
    "            \"n_test_origins\": int(np.mean([r[\"n_test_origins\"] for r in cv_rows if r[\"model\"] == model_name and r[\"outer_fold\"] != \"MEAN\"])) if any((r[\"model\"] == model_name and r[\"outer_fold\"] != \"MEAN\") for r in cv_rows) else 0,\n",
    "            \"used_folds\": int(used_folds),\n",
    "        })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_rows)\n",
    "print(\"\\nOuter-fold day-ahead results (TRAIN backtest):\")\n",
    "display(cv_df)\n",
    "\n",
    "if fold_skips:\n",
    "    skip_df = pd.DataFrame(fold_skips)\n",
    "    print(\"\\nSkipped folds (info):\")\n",
    "    display(skip_df)\n",
    "    skip_df.to_csv(OUT_DIR / \"model_selection_skipped_folds.csv\", index=False)\n",
    "\n",
    "mean_df = cv_df[cv_df[\"outer_fold\"].astype(str) == \"MEAN\"].sort_values(\"dayahead_MAE\")\n",
    "print(\"\\nModel ranking by day-ahead MAE (lower is better):\")\n",
    "display(mean_df[[\"model\",\"dayahead_MAE\",\"dayahead_RMSE\",\"dayahead_R2\",\"fit_seconds\",\"used_folds\"]])\n",
    "\n",
    "best_model_name = mean_df.iloc[0][\"model\"]\n",
    "print(\"\\nSelected BEST model by day-ahead MAE:\", best_model_name)\n",
    "\n",
    "cv_df.to_csv(OUT_DIR / \"model_selection_outer_results.csv\", index=False)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Final tuning on TRAIN (one-step) per model + HOLDOUT day-ahead recursive evaluation\n",
    "# =========================\n",
    "final_rows = []\n",
    "final_curves = {}  # model -> mae_h\n",
    "final_params = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FINAL HOLDOUT EVALUATION (DAY-AHEAD, RECURSIVE)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for model_name, (estimator, param_dist) in models.items():\n",
    "    inner = TimeSeriesSplit(n_splits=INNER_SPLITS)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=max(25, N_ITER),\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=inner,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    search.fit(X_train_full, y_train_full)\n",
    "    fit_s = time.time() - t0\n",
    "\n",
    "    best = search.best_estimator_\n",
    "    final_params[f\"recursive_{model_name}\"] = search.best_params_\n",
    "\n",
    "    overall, mae_h, rmse_h = evaluate_day_ahead_recursive(\n",
    "        model=best,\n",
    "        df_model=df_hold,\n",
    "        exog_cols=exog_cols,\n",
    "        lag_cols=lag_cols,\n",
    "        H=H,\n",
    "        origin_indices=None,\n",
    "        origin_stride=ORIGIN_STRIDE\n",
    "    )\n",
    "\n",
    "    final_curves[f\"Recursive_{model_name}\"] = mae_h\n",
    "\n",
    "    row = {\n",
    "        \"node\": high_node,\n",
    "        \"approach\": \"Recursive\",\n",
    "        \"model\": model_name,\n",
    "        \"MAE\": overall[\"MAE\"],\n",
    "        \"RMSE\": overall[\"RMSE\"],\n",
    "        \"R2\": overall[\"R2\"],\n",
    "        \"Skill_vs_Persistence\": skill_score(overall[\"MAE\"], baseline_overall[\"MAE\"]),\n",
    "        \"fit_seconds\": float(fit_s),\n",
    "        \"best_params\": str(search.best_params_),\n",
    "        **bucket_means(mae_h, H_BUCKETS),\n",
    "    }\n",
    "    final_rows.append(row)\n",
    "\n",
    "final_df = pd.DataFrame(final_rows).sort_values(\"MAE\")\n",
    "print(\"\\nFinal holdout (recursive) — sorted by MAE:\")\n",
    "display(final_df)\n",
    "\n",
    "best_recursive_row = final_df.iloc[0].to_dict()  # best by MAE on holdout recursive\n",
    "final_df.to_csv(OUT_DIR / \"final_holdout_recursive_results.csv\", index=False)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Direct Multi-Horizon (Stacked) only for BEST model (as selected on TRAIN ranking)\n",
    "#     - CV split by ORIGIN\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"DIRECT MULTI-HORIZON (STACKED) for BEST model: {best_model_name}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "if best_model_name == \"Ridge\":\n",
    "    direct_estimator = Pipeline([(\"scaler\", StandardScaler()), (\"model\", Ridge(random_state=RANDOM_STATE))])\n",
    "    direct_space = {\"model__alpha\": np.logspace(-3, 3, 20).tolist()}\n",
    "elif best_model_name == \"RF\":\n",
    "    direct_estimator = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
    "    # already tightened (keep it tighter to avoid explosion)\n",
    "    direct_space = {\n",
    "        \"n_estimators\": [200, 400],\n",
    "        \"max_depth\": [None, 12, 24],\n",
    "        \"min_samples_leaf\": [1, 5],\n",
    "        \"max_features\": [\"sqrt\", 0.8],\n",
    "    }\n",
    "else:\n",
    "    direct_estimator = HistGradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "    direct_space = {\n",
    "        \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "        \"max_depth\": [None, 6, 10, 14],\n",
    "        \"max_leaf_nodes\": [31, 63, 127],\n",
    "        \"min_samples_leaf\": [10, 20, 50],\n",
    "        \"l2_regularization\": [0.0, 0.1, 1.0],\n",
    "    }\n",
    "\n",
    "# build stacked dataset from TRAIN\n",
    "X_direct, y_direct, origin_ids = build_stacked_direct_dataset(\n",
    "    df_model=df_train_full,\n",
    "    exog_cols=exog_cols,\n",
    "    lag_cols=lag_cols,\n",
    "    H=H,\n",
    "    origin_stride=ORIGIN_STRIDE\n",
    ")\n",
    "\n",
    "print(f\"Direct stacked training set: X={X_direct.shape}, y={y_direct.shape}, unique_origins={len(np.unique(origin_ids))}\")\n",
    "\n",
    "direct_cv_splits = make_origin_time_cv_splits(origin_ids, n_splits=INNER_SPLITS)\n",
    "\n",
    "direct_search = RandomizedSearchCV(\n",
    "    estimator=direct_estimator,\n",
    "    param_distributions=direct_space,\n",
    "    n_iter=max(25, N_ITER),\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=direct_cv_splits,           # <-- origin-based CV\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "direct_search.fit(X_direct, y_direct)\n",
    "direct_fit_s = time.time() - t0\n",
    "\n",
    "direct_best = direct_search.best_estimator_\n",
    "final_params[\"direct_best_model\"] = best_model_name\n",
    "final_params[f\"direct_{best_model_name}\"] = direct_search.best_params_\n",
    "\n",
    "direct_overall, direct_mae_h, direct_rmse_h = evaluate_day_ahead_direct(\n",
    "    model=direct_best,\n",
    "    df_model=df_hold,\n",
    "    exog_cols=exog_cols,\n",
    "    lag_cols=lag_cols,\n",
    "    H=H,\n",
    "    origin_stride=ORIGIN_STRIDE\n",
    ")\n",
    "\n",
    "dump(\n",
    "    direct_best, \n",
    "    OUT_DIR / f\"final_model_direct_{best_model_name.lower()}.joblib\"\n",
    ")\n",
    "\n",
    "\n",
    "final_curves[f\"DirectStacked_{best_model_name}\"] = direct_mae_h\n",
    "\n",
    "direct_summary = {\n",
    "    \"node\": high_node,\n",
    "    \"approach\": \"DirectStacked\",\n",
    "    \"model\": best_model_name,\n",
    "    \"MAE\": direct_overall[\"MAE\"],\n",
    "    \"RMSE\": direct_overall[\"RMSE\"],\n",
    "    \"R2\": direct_overall[\"R2\"],\n",
    "    \"Skill_vs_Persistence\": skill_score(direct_overall[\"MAE\"], baseline_overall[\"MAE\"]),\n",
    "    \"fit_seconds\": float(direct_fit_s),\n",
    "    \"best_params\": str(direct_search.best_params_),\n",
    "    **bucket_means(direct_mae_h, H_BUCKETS),\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 6b) Persist: WINNER model (overwrite \"latest\")\n",
    "# =========================\n",
    "\n",
    "# Save winner model + meta\n",
    "winner_info = {\n",
    "    \"winner_approach\": \"DirectStacked\",\n",
    "    \"winner_model_name\": str(best_model_name),\n",
    "    \"H\": int(H),\n",
    "    \"timestamp_col\": TIMESTAMP_COL,\n",
    "    \"target_col\": TARGET,\n",
    "    \"raw_features\": RAW_FEATURES,\n",
    "    \"target_lags\": TARGET_LAGS,\n",
    "    \"fourier_k\": FOURIER_K,\n",
    "    \"period_steps\": PERIOD_STEPS,\n",
    "    \"exog_cols\": exog_cols,\n",
    "    \"lag_cols\": lag_cols,\n",
    "    \"feature_cols_one_step\": feature_cols,\n",
    "    \"direct_h_features\": [\"h\", \"h_over_H\"],\n",
    "    \"baseline_holdout\": baseline_overall,\n",
    "    \"winner_holdout\": {\n",
    "        \"MAE\": float(direct_summary[\"MAE\"]),\n",
    "        \"RMSE\": float(direct_summary[\"RMSE\"]),\n",
    "        \"R2\": float(direct_summary[\"R2\"]),\n",
    "        \"Skill_vs_Persistence\": float(direct_summary[\"Skill_vs_Persistence\"]),\n",
    "    },\n",
    "    \"node_used_for_selection\": str(high_node),\n",
    "    \"note\": \"Auto-saved by notebook. Overwritten each run.\",\n",
    "}\n",
    "\n",
    "dump(direct_best, LATEST_DIR / \"best_model.joblib\")\n",
    "(LATEST_DIR / \"best_model_meta.json\").write_text(\n",
    "    json.dumps(winner_info, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"Saved winner model to:\", (LATEST_DIR / \"best_model.joblib\").resolve())\n",
    "print(\"Saved winner meta  to:\", (LATEST_DIR / \"best_model_meta.json\").resolve())\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Final comparison output (Baseline + Recursive models + Direct best)\n",
    "# =========================\n",
    "baseline_summary = {\n",
    "    \"node\": high_node,\n",
    "    \"approach\": \"Baseline\",\n",
    "    \"model\": \"Persistence\",\n",
    "    \"MAE\": baseline_overall[\"MAE\"],\n",
    "    \"RMSE\": baseline_overall[\"RMSE\"],\n",
    "    \"R2\": baseline_overall[\"R2\"],\n",
    "    \"Skill_vs_Persistence\": 0.0,\n",
    "    \"fit_seconds\": 0.0,\n",
    "    \"best_params\": \"-\",\n",
    "    **bucket_means(baseline_mae_h, H_BUCKETS),\n",
    "}\n",
    "\n",
    "final_compare = pd.DataFrame(\n",
    "    [baseline_summary]\n",
    "    + final_rows\n",
    "    + [direct_summary]\n",
    ").sort_values([\"MAE\",\"RMSE\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FINAL COMPARISON (HOLDOUT, DAY-AHEAD H=100)\")\n",
    "print(\"=\"*90)\n",
    "display(final_compare)\n",
    "\n",
    "final_compare.to_csv(OUT_DIR / \"final_comparison.csv\", index=False)\n",
    "\n",
    "# Save curves to CSV (h, each curve)\n",
    "curves_df = pd.DataFrame({\"h\": np.arange(1, H+1)})\n",
    "curves_df[\"Baseline_Persistence\"] = baseline_mae_h\n",
    "for name, curve in final_curves.items():\n",
    "    curves_df[name] = curve\n",
    "curves_df.to_csv(OUT_DIR / \"mae_by_horizon_curves.csv\", index=False)\n",
    "\n",
    "# Save run config & params\n",
    "run_config = {\n",
    "    \"high_node\": high_node,\n",
    "    \"n_df_model\": int(len(df_model)),\n",
    "    \"n_train\": int(len(df_train_full)),\n",
    "    \"n_holdout\": int(len(df_hold)),\n",
    "    \"H\": H,\n",
    "    \"FINAL_HOLDOUT_FRAC\": FINAL_HOLDOUT_FRAC,\n",
    "    \"OUTER_SPLITS\": OUTER_SPLITS,\n",
    "    \"INNER_SPLITS\": INNER_SPLITS,\n",
    "    \"N_ITER\": N_ITER,\n",
    "    \"ORIGIN_STRIDE\": ORIGIN_STRIDE,\n",
    "    \"MAX_TEST_ORIGINS_PER_FOLD\": MAX_TEST_ORIGINS_PER_FOLD,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"exog_cols\": exog_cols,\n",
    "    \"lag_cols\": lag_cols,\n",
    "    \"present_exog\": present_feats,\n",
    "    \"h_buckets\": H_BUCKETS,\n",
    "}\n",
    "with open(OUT_DIR / \"run_config.json\", \"w\") as f:\n",
    "    json.dump(run_config, f, indent=2)\n",
    "\n",
    "with open(OUT_DIR / \"best_params.json\", \"w\") as f:\n",
    "    json.dump(final_params, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved artifacts to: {OUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plot: MAE(h) curves\n",
    "# -------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(np.arange(1, H+1), baseline_mae_h, label=\"Baseline: Persistence\")\n",
    "\n",
    "for r in final_rows:\n",
    "    key = f\"Recursive_{r['model']}\"\n",
    "    plt.plot(np.arange(1, H+1), final_curves[key], label=f\"Recursive: {r['model']}\")\n",
    "\n",
    "plt.plot(np.arange(1, H+1), direct_mae_h, linewidth=2.5, label=f\"DirectStacked: {best_model_name}\")\n",
    "\n",
    "plt.title(f\"Day-Ahead Error vs Horizon (MAE(h)) — Node: {high_node} — H={H}\")\n",
    "plt.xlabel(\"Horizon h (steps ahead)\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest model (TRAIN ranking):\", best_model_name)\n",
    "print(\"Baseline (holdout):\", baseline_overall)\n",
    "print(\"DirectStacked (holdout):\", {k: direct_summary[k] for k in [\"MAE\",\"RMSE\",\"R2\",\"Skill_vs_Persistence\"]})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
